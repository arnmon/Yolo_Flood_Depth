{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f780b802-a445-4b71-bae8-bd4ce5d5cb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "from collections import Counter, defaultdict\n",
    "import yaml\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIG (keep your paths/flow)\n",
    "# ---------------------------\n",
    "DATA_ROOT = \"/home/arnab/Desktop/yolo/data/Flood_model/yolov11_refined_balanced\"\n",
    "\n",
    "IMG_DIRS = [\n",
    "    f\"{DATA_ROOT}/train/images\",\n",
    "    f\"{DATA_ROOT}/valid/images\",\n",
    "    f\"{DATA_ROOT}/test/images\",\n",
    "]\n",
    "\n",
    "LBL_DIRS = [\n",
    "    f\"{DATA_ROOT}/train/labels\",\n",
    "    f\"{DATA_ROOT}/valid/labels\",\n",
    "    f\"{DATA_ROOT}/test/labels\",\n",
    "]\n",
    "\n",
    "# ---- Output folder ----\n",
    "OUT_FOLDER = f\"{DATA_ROOT}/object_label_csv2\"\n",
    "os.makedirs(OUT_FOLDER, exist_ok=True)\n",
    "\n",
    "OUT_CSV = f\"{OUT_FOLDER}/object_and_labels.csv\"\n",
    "\n",
    "YOLO_DET_PATH = \"yolo11m.pt\"\n",
    "DET_CONF_TH = 0.25\n",
    "\n",
    "# ---------------------------\n",
    "# LOAD Roboflow data.yaml and build index->numeric-level map\n",
    "# ---------------------------\n",
    "DATA_YAML = Path(f\"{DATA_ROOT}/data.yaml\")\n",
    "idx_to_level = {}  # map label-index (int in txt) -> numeric flood level (int from 'Level X')\n",
    "\n",
    "if DATA_YAML.exists():\n",
    "    with open(DATA_YAML, \"r\") as fh:\n",
    "        dy = yaml.safe_load(fh)\n",
    "    rf_names = dy.get(\"names\", [])\n",
    "    # rf_names can be list like ['Level 0','Level 1','Level 10','Level 2', ...]\n",
    "    for idx, name in enumerate(rf_names):\n",
    "        # extract digits from the name string to get numeric flood level\n",
    "        try:\n",
    "            # pick continuous digits groups (handles 'Level 10' etc.)\n",
    "            digits = ''.join(ch for ch in str(name) if ch.isdigit())\n",
    "            num = int(digits) if digits != \"\" else idx\n",
    "        except Exception:\n",
    "            num = idx\n",
    "        idx_to_level[int(idx)] = int(num)\n",
    "else:\n",
    "    # fallback: identity mapping (index -> index) so script still runs\n",
    "    print(f\"Warning: {DATA_YAML} not found. Using identity mapping index->level as fallback.\")\n",
    "    idx_to_level = {}  # leave empty, will fallback to lab_idx wherever used\n",
    "\n",
    "# small sanity print (show mapping or note if empty)\n",
    "if len(idx_to_level) > 0:\n",
    "    print(\"Roboflow index -> flood_level mapping (sample):\")\n",
    "    for k in sorted(list(idx_to_level.keys()))[:20]:\n",
    "        print(f\"  index {k} -> flood_level {idx_to_level[k]}\")\n",
    "else:\n",
    "    print(\"Roboflow mapping empty — script will use label-index as numeric flood level (lab_idx).\")\n",
    "\n",
    "# ---------------------------\n",
    "# LOAD YOLO DETECTION MODEL\n",
    "# ---------------------------\n",
    "det_model = YOLO(YOLO_DET_PATH)\n",
    "\n",
    "\n",
    "def detect_object_name(crop):\n",
    "    \"\"\"Return YOLO detected class name for the crop.\"\"\"\n",
    "    try:\n",
    "        res = det_model.predict(crop, conf=DET_CONF_TH, imgsz=256, verbose=False)[0]\n",
    "        if getattr(res, \"boxes\", None) is None or len(res.boxes) == 0:\n",
    "            return \"unknown\"\n",
    "        idx = res.boxes.conf.argmax()\n",
    "        cls_id = int(res.boxes.cls[idx].item())\n",
    "        cname = det_model.model.names[cls_id]\n",
    "        return cname\n",
    "    except Exception:\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# QUICK PRE-SCAN: count labels in .txt files (expected counts)\n",
    "# Note: now we map label index -> numeric flood level using idx_to_level\n",
    "# ---------------------------\n",
    "expected_counts = Counter()\n",
    "total_label_lines = 0\n",
    "label_files_scanned = 0\n",
    "\n",
    "for lbl_dir in LBL_DIRS:\n",
    "    if not os.path.isdir(lbl_dir):\n",
    "        continue\n",
    "    for fname in os.listdir(lbl_dir):\n",
    "        if not fname.lower().endswith(\".txt\"):\n",
    "            continue\n",
    "        lbl_path = os.path.join(lbl_dir, fname)\n",
    "        with open(lbl_path, \"r\") as f:\n",
    "            lines = [ln.strip() for ln in f.read().splitlines() if ln.strip()]\n",
    "        label_files_scanned += 1\n",
    "        for ln in lines:\n",
    "            parts = ln.split()\n",
    "            if len(parts) < 1:\n",
    "                continue\n",
    "            try:\n",
    "                lab_idx = int(parts[0])\n",
    "                # map index -> numeric flood level via data.yaml mapping (safe fallback to lab_idx)\n",
    "                flood_level = idx_to_level.get(lab_idx, lab_idx)\n",
    "                expected_counts[flood_level] += 1\n",
    "                total_label_lines += 1\n",
    "            except Exception:\n",
    "                # skip malformed label lines\n",
    "                continue\n",
    "\n",
    "print(f\"Pre-scan: scanned {label_files_scanned} label files, total label lines = {total_label_lines}\")\n",
    "print(\"Expected label counts from .txt files (label-only, mapped via data.yaml):\")\n",
    "for lvl in sorted(expected_counts.keys()):\n",
    "    print(f\"  Level {lvl}: {expected_counts[lvl]}\")\n",
    "\n",
    "# ---------------------------\n",
    "# MAIN EXTRACTION (preserve original flow)\n",
    "# ---------------------------\n",
    "rows = []\n",
    "# We'll also keep a map of img -> number of label lines (for tracing)\n",
    "img_label_line_counts = defaultdict(int)\n",
    "\n",
    "for split_idx, (img_dir, lbl_dir) in enumerate(zip(IMG_DIRS, LBL_DIRS), start=1):\n",
    "    # preserve original listing order (no forced sorting)\n",
    "    images = [f for f in os.listdir(img_dir) if f.lower().endswith((\".jpg\", \".png\"))]\n",
    "\n",
    "    for img_index, img_name in enumerate(images):\n",
    "        img_path = os.path.join(img_dir, img_name)\n",
    "        lbl_path = os.path.join(lbl_dir, Path(img_name).stem + \".txt\")\n",
    "\n",
    "        if not os.path.exists(lbl_path):\n",
    "            continue\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        h, w = img.shape[:2]\n",
    "\n",
    "        with open(lbl_path, \"r\") as f:\n",
    "            lines = [ln.strip() for ln in f.read().splitlines() if ln.strip()]\n",
    "\n",
    "        # track how many label lines existed for this image (for debugging)\n",
    "        img_label_line_counts[img_path] = len(lines)\n",
    "\n",
    "        for line_idx, ln in enumerate(lines):\n",
    "            parts = ln.split()\n",
    "            if len(parts) != 5:\n",
    "                # keep behavior: only accept 5-part lines\n",
    "                continue\n",
    "\n",
    "            # parse flood level via Roboflow mapping:\n",
    "            try:\n",
    "                lab_idx = int(parts[0])\n",
    "                flood_level = idx_to_level.get(lab_idx, lab_idx)  # safe fallback to lab_idx\n",
    "            except Exception:\n",
    "                # skip malformed flood level\n",
    "                continue\n",
    "\n",
    "            # bbox decode (same as your original)\n",
    "            try:\n",
    "                cx, cy, bw, bh = map(float, parts[1:])\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            cx *= w; cy *= h; bw *= w; bh *= h\n",
    "\n",
    "            x1 = int(cx - bw/2)\n",
    "            y1 = int(cy - bh/2)\n",
    "            x2 = int(cx + bw/2)\n",
    "            y2 = int(cy + bh/2)\n",
    "\n",
    "            x1 = max(0, x1); y1 = max(0, y1)\n",
    "            x2 = min(w-1, x2); y2 = min(h-1, y2)\n",
    "\n",
    "            if x2 <= x1 or y2 <= y1:\n",
    "                continue\n",
    "\n",
    "            crop = img[y1:y2, x1:x2]\n",
    "\n",
    "            object_name = detect_object_name(crop)\n",
    "\n",
    "            # keep trace columns so we can later map any row back to exact file/line\n",
    "            rows.append({\n",
    "                \"object_name\": object_name.lower(),\n",
    "                \"flood_level\": int(flood_level),\n",
    "                \"image_path\": img_path,\n",
    "                \"split\": (\"train\" if split_idx==1 else \"valid\" if split_idx==2 else \"test\"),\n",
    "                \"img_index_in_dir\": img_index,\n",
    "                \"label_line_idx\": line_idx,   # which line in the label file this came from\n",
    "                \"bbox_x1\": x1, \"bbox_y1\": y1, \"bbox_x2\": x2, \"bbox_y2\": y2\n",
    "            })\n",
    "\n",
    "# ---------------------------\n",
    "# SAVE CSV\n",
    "# ---------------------------\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(\"CSV SAVED:\", OUT_CSV)\n",
    "print(\"Total object rows written:\", len(df))\n",
    "\n",
    "# ---------------------------\n",
    "# DIAGNOSTIC REPORT: compare expected (.txt) counts vs produced CSV counts\n",
    "# ---------------------------\n",
    "produced_counts = df[\"flood_level\"].value_counts().to_dict()\n",
    "\n",
    "print(\"\\nProduced CSV flood level counts (from final CSV):\")\n",
    "for lvl in sorted(produced_counts.keys()):\n",
    "    print(f\"  Level {lvl}: {produced_counts[lvl]}\")\n",
    "\n",
    "# Compare (show diff = produced - expected)\n",
    "print(\"\\nDIFFERENCE (produced - expected):\")\n",
    "all_lvls = sorted(set(list(expected_counts.keys()) + list(produced_counts.keys())))\n",
    "for lvl in all_lvls:\n",
    "    exp = expected_counts.get(lvl, 0)\n",
    "    prod = produced_counts.get(lvl, 0)\n",
    "    diff = prod - exp\n",
    "    note = \"\"\n",
    "    if diff != 0:\n",
    "        note = \" <-- MISMATCH\"\n",
    "    print(f\"  Level {lvl}: produced={prod} expected={exp} diff={diff}{note}\")\n",
    "\n",
    "# Extra trace: list any images where number of produced rows != number of label lines\n",
    "img_rows_count = df.groupby(\"image_path\").size().to_dict()\n",
    "mismatched_images = []\n",
    "for img_path, lbl_count in img_label_line_counts.items():\n",
    "    prod_rows = img_rows_count.get(img_path, 0)\n",
    "    if prod_rows != lbl_count:\n",
    "        mismatched_images.append((img_path, lbl_count, prod_rows))\n",
    "\n",
    "if mismatched_images:\n",
    "    print(\"\\nImages where produced rows != label lines (image_path, label_lines, produced_rows):\")\n",
    "    for t in mismatched_images[:50]:\n",
    "        print(\" \", t)\n",
    "else:\n",
    "    print(\"\\nAll images produced same number of object rows as label lines (good).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6444c0-8c16-4438-bdbb-87c45825def1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "CSV_PATH = \"/home/arnab/Desktop/yolo/data/Flood_model/yolov11_refined_balanced/object_label_csv2/object_and_labels.csv\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# group by object_name and flood_level\n",
    "grouped = df.groupby([\"object_name\", \"flood_level\"]).size().reset_index(name=\"count\")\n",
    "\n",
    "# print nicely\n",
    "for obj in grouped[\"object_name\"].unique():\n",
    "    print(f\"\\n=== OBJECT: {obj} ===\")\n",
    "    sub = grouped[grouped[\"object_name\"] == obj].sort_values(\"flood_level\")\n",
    "    for _, row in sub.iterrows():\n",
    "        level = row[\"flood_level\"]\n",
    "        cnt = row[\"count\"]\n",
    "        print(f\"  Level {level}: {cnt} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48e5955-b992-4c72-ac35-6caacbc22314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "DATA_ROOT = \"/home/arnab/Desktop/yolo/data/Flood_model/yolov11_refined_balanced\"\n",
    "\n",
    "CSV_PATH = f\"{DATA_ROOT}/object_label_csv2/object_and_labels.csv\"\n",
    "\n",
    "IMG_DIRS = [\n",
    "    f\"{DATA_ROOT}/train/images\",\n",
    "    f\"{DATA_ROOT}/valid/images\",\n",
    "    f\"{DATA_ROOT}/test/images\",\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Count unique image paths in CSV\n",
    "# -----------------------------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "unique_csv_images = df[\"image_path\"].unique()\n",
    "num_unique_csv_images = len(unique_csv_images)\n",
    "\n",
    "print(\"Unique images in CSV:\", num_unique_csv_images)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Count actual images in dataset folders\n",
    "# -----------------------------\n",
    "actual_images = []\n",
    "\n",
    "for d in IMG_DIRS:\n",
    "    for f in os.listdir(d):\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            actual_images.append(os.path.join(d, f))\n",
    "\n",
    "actual_images = sorted(actual_images)\n",
    "num_actual_images = len(actual_images)\n",
    "\n",
    "print(\"Actual images in dataset:\", num_actual_images)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Compare\n",
    "# -----------------------------\n",
    "if num_unique_csv_images == num_actual_images:\n",
    "    print(\"\\n✅ MATCH: CSV contains all images.\")\n",
    "else:\n",
    "    print(\"\\n❌ MISMATCH:\")\n",
    "    print(\"Images missing in CSV:\", num_actual_images - num_unique_csv_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003b06eb-15ab-4507-a7bf-ff9cab6a0d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "CSV_PATH = \"/home/arnab/Desktop/yolo/data/Flood_model/yolov11_refined_balanced/object_label_csv2/object_and_labels_filtered.csv\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "unique_classes = df[\"object_name\"].unique()\n",
    "unique_levels = df[\"flood_level\"].unique()\n",
    "\n",
    "print(\"UNIQUE OBJECT CLASSES:\")\n",
    "for c in unique_classes:\n",
    "    print(\" -\", c)\n",
    "\n",
    "print(\"\\nFLOOD LEVEL COUNTS:\")\n",
    "level_counts = df[\"flood_level\"].value_counts().sort_index()\n",
    "\n",
    "for level, count in level_counts.items():\n",
    "    print(f\"Level {level}: {count} samples\")\n",
    "\n",
    "print(\"\\nTOTAL UNIQUE FLOOD LEVELS:\", len(unique_levels))\n",
    "print(\"TOTAL SAMPLES:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364b364b-3004-404c-adba-2e42d1a3eb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "CSV_IN = \"/home/arnab/Desktop/yolo/data/Flood_model/yolov11_refined_balanced/object_label_csv2/object_and_labels.csv\"\n",
    "CSV_OUT = \"/home/arnab/Desktop/yolo/data/Flood_model/yolov11_refined_balanced/object_label_csv2/object_and_labels_filtered.csv\"\n",
    "\n",
    "# classes to keep\n",
    "KEEP_CLASSES = [\"person\", \"motorcycle\", \"car\", \"truck\", \"bus\", \"bicycle\"]\n",
    "\n",
    "df = pd.read_csv(CSV_IN)\n",
    "\n",
    "# keep rows with allowed object names\n",
    "df_filtered = df[df[\"object_name\"].isin(KEEP_CLASSES)].copy()\n",
    "\n",
    "df_filtered.to_csv(CSV_OUT, index=False)\n",
    "\n",
    "print(\"Filtered CSV saved to:\", CSV_OUT)\n",
    "print(\"Rows kept:\", len(df_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992fdedd-fb9e-482d-940c-6296ce954247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script Final — Compute physics + semantics per GT from object_and_labels_filtered.csv\n",
    " - Uses filtered CSV (no re-detection)\n",
    " - Per-image: water segmentation + pose (if weights present)\n",
    " - Per-GT: compute base features + semantic features for 6 classes:\n",
    "    person, car, truck, bus, motorcycle, bicycle\n",
    " - Saves CSV with original columns plus features\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "import yaml\n",
    "\n",
    "# -----------------------\n",
    "# CONFIG (edit if needed)\n",
    "# -----------------------\n",
    "DATA_ROOT = \"/home/arnab/Desktop/yolo/data/Flood_model/yolov11_refined_balanced\"\n",
    "IN_FILTERED_CSV = f\"{DATA_ROOT}/object_label_csv2/object_and_labels_filtered.csv\"\n",
    "OUT_FOLDER = f\"{DATA_ROOT}/object_label_csv2\"\n",
    "os.makedirs(OUT_FOLDER, exist_ok=True)\n",
    "OUT_CSV = f\"{OUT_FOLDER}/object_and_labels_final_44feat.csv\"\n",
    "\n",
    "IMG_DIRS = {\n",
    "    \"train\": f\"{DATA_ROOT}/train/images\",\n",
    "    \"valid\": f\"{DATA_ROOT}/valid/images\",\n",
    "    \"test\":  f\"{DATA_ROOT}/test/images\",\n",
    "}\n",
    "\n",
    "LBL_DIRS = {\n",
    "    \"train\": f\"{DATA_ROOT}/train/labels\",\n",
    "    \"valid\": f\"{DATA_ROOT}/valid/labels\",\n",
    "    \"test\":  f\"{DATA_ROOT}/test/labels\",\n",
    "}\n",
    "\n",
    "# models (change paths if needed)\n",
    "YOLO_POSE_PATH = \"yolo11m-pose.pt\"   # optional\n",
    "YOLO_WSEG_PATH = \"/home/arnab/Desktop/yolo/data/Flood_model/UrbanFlood_WaterSeg2/yolo11m-water-seg2/weights/best.pt\"  # optional\n",
    "\n",
    "# thresholds\n",
    "POSE_CONF_TH = 0.25\n",
    "WATER_CONF_TH = 0.25\n",
    "\n",
    "# -----------------------\n",
    "# data.yaml mapping: index -> numeric flood level\n",
    "# -----------------------\n",
    "DATA_YAML = Path(f\"{DATA_ROOT}/data.yaml\")\n",
    "idx_to_level = {}\n",
    "if DATA_YAML.exists():\n",
    "    try:\n",
    "        with open(DATA_YAML, \"r\") as fh:\n",
    "            dy = yaml.safe_load(fh)\n",
    "        rf_names = dy.get(\"names\", [])\n",
    "        for idx, name in enumerate(rf_names):\n",
    "            try:\n",
    "                digits = ''.join(ch for ch in str(name) if ch.isdigit())\n",
    "                num = int(digits) if digits != \"\" else idx\n",
    "            except Exception:\n",
    "                num = idx\n",
    "            idx_to_level[int(idx)] = int(num)\n",
    "    except Exception:\n",
    "        idx_to_level = {}\n",
    "else:\n",
    "    idx_to_level = {}\n",
    "\n",
    "if len(idx_to_level) > 0:\n",
    "    print(\"Loaded data.yaml mapping (index -> flood level) sample:\")\n",
    "    for k in sorted(list(idx_to_level.keys()))[:20]:\n",
    "        print(f\"  {k} -> {idx_to_level[k]}\")\n",
    "else:\n",
    "    print(\"No data.yaml mapping found — will use label index as numeric flood level (lab_idx).\")\n",
    "\n",
    "# height priors (cm) — UPDATED per your request\n",
    "CLASS_HEIGHT_PRIORS_CM = {\n",
    "    \"person\":170, \"car\":150, \"bus\":300, \"truck\":300,\n",
    "    \"bicycle\":100, \"motorcycle\":110,\n",
    "}\n",
    "DEFAULT_CLASS_HEIGHT = 150\n",
    "\n",
    "# Allowed classes (we keep only these six in semantics)\n",
    "ALLOWED_CLASSES = [\"person\",\"car\",\"truck\",\"bus\",\"motorcycle\",\"bicycle\"]\n",
    "\n",
    "# -----------------------\n",
    "# FEATURE LIST (includes geometry, water, physics, and per-class semantic groups)\n",
    "# NOTE: human_* renamed to person_* as requested\n",
    "# -----------------------\n",
    "ALL_FEATURES = [\n",
    "    # geometry\n",
    "    \"box_x1\",\"box_y1\",\"box_x2\",\"box_y2\",\"box_w\",\"box_h\",\"box_area\",\n",
    "    # water/base\n",
    "    \"water_pixels\",\"water_area_frac\",\"water_top_y\",\"water_height_frac\",\n",
    "    \"sgf_mean\",\"sgf_max\",\"WOMI\",\"submergence_ratio\",\"bbox_ratio\",\n",
    "    \"water_frac_ratio\",\"waterline_norm\",\n",
    "    # physics\n",
    "    \"estimated_depth_cm\",\"ref_height_cm\",\"physics_residual\",\n",
    "    # person semantics\n",
    "    \"person_sub_ankle\",\"person_sub_knee\",\"person_sub_hip\",\"person_sub_chest\",\"person_sub_head\",\"person_depth_norm\",\n",
    "    # NEW person fine-grained semantics\n",
    "    \"person_sub_mid_thigh\",\"person_sub_upper_thigh\",\"person_sub_lower_waist\",\"person_depth_fine_ratio\",\n",
    "    # generic submergence flags\n",
    "    \"obj_sub_20pct\",\"obj_sub_50pct\",\"obj_sub_80pct\",\n",
    "    # car semantics\n",
    "    \"car_sub_wheel\",\"car_sub_mid\",\"car_sub_window\",\"car_sub_roof\",\"car_depth_norm\",\"car_sub_half_door\",\n",
    "    # bus semantics\n",
    "    \"bus_sub_wheel\",\"bus_sub_mid\",\"bus_sub_window\",\"bus_sub_roof\",\"bus_depth_norm\",\n",
    "    # truck semantics\n",
    "    \"truck_sub_wheel\",\"truck_sub_mid\",\"truck_sub_window\",\"truck_sub_roof\",\"truck_depth_norm\",\"truck_sub_hubcap\",\n",
    "    # motorcycle semantics\n",
    "    \"motorcycle_sub_wheel\",\"motorcycle_sub_seat\",\"motorcycle_sub_handle\",\"motorcycle_depth_norm\",\"motorcycle_sub_engine_level\",\n",
    "    # bicycle semantics\n",
    "    \"bicycle_sub_wheel\",\"bicycle_sub_seat\",\"bicycle_sub_handle\",\"bicycle_depth_norm\"\n",
    "]\n",
    "\n",
    "# -----------------------\n",
    "# UTILITIES\n",
    "# -----------------------\n",
    "def safe_name(s):\n",
    "    if s is None:\n",
    "        return \"unknown\"\n",
    "    return str(s).strip().lower()\n",
    "\n",
    "def nanmean_safe(values, fallback):\n",
    "    arr = []\n",
    "    for v in values:\n",
    "        if v is None:\n",
    "            arr.append(np.nan)\n",
    "        else:\n",
    "            try:\n",
    "                arr.append(float(v))\n",
    "            except:\n",
    "                arr.append(np.nan)\n",
    "    out = np.nanmean(arr)\n",
    "    if np.isnan(out):\n",
    "        return fallback\n",
    "    return out\n",
    "\n",
    "def compute_iou(b1, b2):\n",
    "    x1 = max(b1[0], b2[0]); y1 = max(b1[1], b2[1])\n",
    "    x2 = min(b1[2], b2[2]); y2 = min(b1[3], b2[3])\n",
    "    inter = max(0, x2-x1) * max(0, y2-y1)\n",
    "    if inter <= 0: return 0.0\n",
    "    a1 = max(1e-9, (b1[2]-b1[0])*(b1[3]-b1[1])); a2 = max(1e-9, (b2[2]-b2[0])*(b2[3]-b2[1]))\n",
    "    return inter / (a1 + a2 - inter + 1e-9)\n",
    "\n",
    "# -----------------------\n",
    "# Models: load pose & wseg (optional)\n",
    "# -----------------------\n",
    "def load_models():\n",
    "    pose = YOLO(YOLO_POSE_PATH) if os.path.exists(YOLO_POSE_PATH) else None\n",
    "    wseg = YOLO(YOLO_WSEG_PATH) if os.path.exists(YOLO_WSEG_PATH) else None\n",
    "    return pose, wseg\n",
    "\n",
    "def run_water_seg(wseg_model, image):\n",
    "    try:\n",
    "        if wseg_model is None:\n",
    "            return np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "        res = wseg_model.predict(image, imgsz=1024, conf=WATER_CONF_TH, verbose=False)[0]\n",
    "        if getattr(res, \"masks\", None) is None:\n",
    "            return np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "        mask = res.masks.data.cpu().numpy()\n",
    "        if mask.ndim == 3:\n",
    "            mask = (mask > 0.5).any(axis=0)\n",
    "        else:\n",
    "            mask = (mask > 0.5)\n",
    "        return mask.astype(np.uint8)\n",
    "    except Exception:\n",
    "        return np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "\n",
    "def extract_poses(pose_model, image):\n",
    "    if pose_model is None:\n",
    "        return []\n",
    "    try:\n",
    "        res = pose_model.predict(image, imgsz=640, conf=POSE_CONF_TH, verbose=False)[0]\n",
    "        if getattr(res, \"keypoints\", None) is None:\n",
    "            return []\n",
    "        return res.keypoints.data.cpu().numpy()\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def nearest_pose(box, poses):\n",
    "    if len(poses) == 0:\n",
    "        return None\n",
    "    x1, y1, x2, y2 = box\n",
    "    cx = (x1 + x2) / 2; cy = (y1 + y2) / 2\n",
    "    best_pose = None; best_d = 1e12\n",
    "    for p in poses:\n",
    "        try:\n",
    "            xs = p[:,0]; ys = p[:,1]\n",
    "            xs = xs[~np.isnan(xs)]; ys = ys[~np.isnan(ys)]\n",
    "            if len(xs) == 0:\n",
    "                px, py = cx, cy\n",
    "            else:\n",
    "                px, py = xs.mean(), ys.mean()\n",
    "        except:\n",
    "            px, py = cx, cy\n",
    "        d = (px - cx)**2 + (py - cy)**2\n",
    "        if d < best_d:\n",
    "            best_d = d; best_pose = p\n",
    "    return best_pose\n",
    "\n",
    "# -----------------------\n",
    "# Compute base physics features for single box\n",
    "# -----------------------\n",
    "def compute_base_features(box, mask, img_w, img_h):\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    bw = max(1, x2 - x1); bh = max(1, y2 - y1)\n",
    "    box_area = bw * bh\n",
    "    if (y2 > y1) and (x2 > x1):\n",
    "        submask = mask[y1:y2, x1:x2]\n",
    "    else:\n",
    "        submask = np.zeros((0,0), dtype=np.uint8)\n",
    "    water_pixels = int(submask.sum())\n",
    "    water_area_frac = water_pixels / (box_area + 1e-9)\n",
    "    ys = np.where(submask > 0)[0]\n",
    "    if len(ys) > 0:\n",
    "        top_local = ys.min() + y1\n",
    "        water_height_frac = (y2 - top_local) / (bh + 1e-9)\n",
    "    else:\n",
    "        top_local = y2\n",
    "        water_height_frac = 0.0\n",
    "    if submask.size > 0:\n",
    "        sm = (submask * 255).astype(np.uint8)\n",
    "        gx = cv2.Sobel(sm, cv2.CV_32F, 1, 0, ksize=3)\n",
    "        gy = cv2.Sobel(sm, cv2.CV_32F, 0, 1, ksize=3)\n",
    "        grad = np.sqrt(gx*gx + gy*gy)\n",
    "        sgf_mean = float(grad.mean()); sgf_max = float(grad.max())\n",
    "    else:\n",
    "        sgf_mean, sgf_max = 0.0, 0.0\n",
    "    water_top_y = int(top_local)\n",
    "    return {\n",
    "        \"box_x1\": x1, \"box_y1\": y1, \"box_x2\": x2, \"box_y2\": y2,\n",
    "        \"box_w\": bw, \"box_h\": bh, \"box_area\": box_area,\n",
    "        \"water_pixels\": water_pixels, \"water_area_frac\": water_area_frac,\n",
    "        \"water_top_y\": water_top_y, \"water_height_frac\": water_height_frac,\n",
    "        \"sgf_mean\": sgf_mean, \"sgf_max\": sgf_max, \"WOMI\": water_area_frac,\n",
    "        \"submergence_ratio\": water_height_frac, \"bbox_ratio\": bh / (bw + 1e-9),\n",
    "        \"water_frac_ratio\": water_area_frac, \"waterline_norm\": water_top_y / (img_h + 1e-9)\n",
    "    }\n",
    "\n",
    "# -----------------------\n",
    "# Semantics and vehicle/person blocks (unchanged logic, renamed human->person)\n",
    "# -----------------------\n",
    "def person_semantics(pose, water_top_y, box):\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    bh = max(1, y2 - y1)\n",
    "    ankle_b = y1 + 0.95 * bh; knee_b  = y1 + 0.75 * bh\n",
    "    hip_b   = y1 + 0.55 * bh; chest_b = y1 + 0.35 * bh; head_b  = y1 + 0.15 * bh\n",
    "    mid_thigh_b   = y1 + 0.80 * bh\n",
    "    upper_thigh_b = y1 + 0.70 * bh\n",
    "    lower_waist_b = y1 + 0.60 * bh\n",
    "    ankle, knee, hip, chest, head = ankle_b, knee_b, hip_b, chest_b, head_b\n",
    "    if pose is not None:\n",
    "        try:\n",
    "            conf = pose[:,2]\n",
    "        except:\n",
    "            conf = np.zeros(pose.shape[0])\n",
    "        def gety(idx):\n",
    "            try:\n",
    "                if conf[idx] > POSE_CONF_TH:\n",
    "                    return float(pose[idx,1])\n",
    "            except:\n",
    "                return None\n",
    "        ankle = nanmean_safe([gety(15), gety(16)], ankle_b)\n",
    "        knee  = nanmean_safe([gety(13), gety(14)], knee_b)\n",
    "        hip   = nanmean_safe([gety(11), gety(12)], hip_b)\n",
    "        chest = nanmean_safe([gety(5),  gety(6)],  chest_b)\n",
    "        head  = nanmean_safe([gety(0)],            head_b)\n",
    "    flags = {\n",
    "        \"person_sub_ankle\": int(water_top_y > ankle),\n",
    "        \"person_sub_knee\":  int(water_top_y > knee),\n",
    "        \"person_sub_hip\":   int(water_top_y > hip),\n",
    "        \"person_sub_chest\": int(water_top_y > chest),\n",
    "        \"person_sub_head\":  int(water_top_y > head),\n",
    "    }\n",
    "    flags.update({\n",
    "        \"person_sub_mid_thigh\":   int(water_top_y > mid_thigh_b),\n",
    "        \"person_sub_upper_thigh\": int(water_top_y > upper_thigh_b),\n",
    "        \"person_sub_lower_waist\": int(water_top_y > lower_waist_b),\n",
    "    })\n",
    "    denom = head - ankle\n",
    "    if denom <= 0:\n",
    "        fine_ratio = 0.0\n",
    "    else:\n",
    "        fine_ratio = (water_top_y - lower_waist_b) / denom\n",
    "        fine_ratio = float(np.clip(fine_ratio, 0.0, 1.0))\n",
    "    flags[\"person_depth_fine_ratio\"] = fine_ratio\n",
    "    if denom <= 0:\n",
    "        depth = 0.0\n",
    "    else:\n",
    "        depth = (water_top_y - ankle) / denom\n",
    "        depth = float(np.clip(depth, 0.0, 1.0))\n",
    "    flags[\"person_depth_norm\"] = depth\n",
    "    return flags\n",
    "\n",
    "def car_semantics(box, water_top_y):\n",
    "    x1,y1,x2,y2 = map(int, box)\n",
    "    bh = max(1, y2 - y1)\n",
    "    wheel = y1 + int(0.85*bh); mid = y1 + int(0.50*bh)\n",
    "    window = y1 + int(0.65*bh); roof = y1\n",
    "    half_door = y1 + int(0.45 * bh)\n",
    "    flags = {\n",
    "        \"car_sub_wheel\": int(water_top_y > wheel),\n",
    "        \"car_sub_mid\": int(water_top_y > mid),\n",
    "        \"car_sub_window\": int(water_top_y > window),\n",
    "        \"car_sub_roof\": int(water_top_y > roof),\n",
    "        \"car_sub_half_door\": int(water_top_y > half_door),\n",
    "    }\n",
    "    denom = roof - wheel\n",
    "    if denom <= 0:\n",
    "        flags[\"car_depth_norm\"] = 0.0\n",
    "    else:\n",
    "        flags[\"car_depth_norm\"] = float(np.clip((water_top_y - wheel)/denom, 0.0, 1.0))\n",
    "    return flags\n",
    "\n",
    "def bus_semantics(box, water_top_y):\n",
    "    x1,y1,x2,y2 = map(int, box)\n",
    "    bh = max(1, y2 - y1)\n",
    "    wheel = y1 + int(0.85*bh); mid = y1 + int(0.50*bh)\n",
    "    window = y1 + int(0.65*bh); roof = y1\n",
    "    flags = {\n",
    "        \"bus_sub_wheel\": int(water_top_y > wheel),\n",
    "        \"bus_sub_mid\": int(water_top_y > mid),\n",
    "        \"bus_sub_window\": int(water_top_y > window),\n",
    "        \"bus_sub_roof\": int(water_top_y > roof)\n",
    "    }\n",
    "    denom = roof - wheel\n",
    "    if denom <= 0:\n",
    "        flags[\"bus_depth_norm\"] = 0.0\n",
    "    else:\n",
    "        flags[\"bus_depth_norm\"] = float(np.clip((water_top_y - wheel)/denom, 0.0, 1.0))\n",
    "    return flags\n",
    "\n",
    "def truck_semantics(box, water_top_y):\n",
    "    x1,y1,x2,y2 = map(int, box)\n",
    "    bh = max(1, y2 - y1)\n",
    "    wheel = y1 + int(0.85*bh); mid = y1 + int(0.50*bh)\n",
    "    window = y1 + int(0.65*bh); roof = y1\n",
    "    hubcap = y1 + int(0.90 * bh)\n",
    "    flags = {\n",
    "        \"truck_sub_wheel\": int(water_top_y > wheel),\n",
    "        \"truck_sub_mid\": int(water_top_y > mid),\n",
    "        \"truck_sub_window\": int(water_top_y > window),\n",
    "        \"truck_sub_roof\": int(water_top_y > roof),\n",
    "        \"truck_sub_hubcap\": int(water_top_y > hubcap),\n",
    "    }\n",
    "    denom = roof - wheel\n",
    "    if denom <= 0:\n",
    "        flags[\"truck_depth_norm\"] = 0.0\n",
    "    else:\n",
    "        flags[\"truck_depth_norm\"] = float(np.clip((water_top_y - wheel)/denom, 0.0, 1.0))\n",
    "    return flags\n",
    "\n",
    "def motorcycle_semantics(box, water_top_y):\n",
    "    x1,y1,x2,y2 = map(int, box)\n",
    "    bh = max(1, y2 - y1)\n",
    "    wheel = y1 + int(0.90*bh); seat = y1 + int(0.60*bh); handle = y1 + int(0.30*bh)\n",
    "    engine = y1 + int(0.50 * bh)\n",
    "    flags = {\n",
    "        \"motorcycle_sub_wheel\": int(water_top_y > wheel),\n",
    "        \"motorcycle_sub_seat\": int(water_top_y > seat),\n",
    "        \"motorcycle_sub_handle\": int(water_top_y > handle),\n",
    "        \"motorcycle_sub_engine_level\": int(water_top_y > engine),\n",
    "    }\n",
    "    denom = handle - wheel\n",
    "    if denom <= 0:\n",
    "        flags[\"motorcycle_depth_norm\"] = 0.0\n",
    "    else:\n",
    "        flags[\"motorcycle_depth_norm\"] = float(np.clip((water_top_y - wheel)/denom, 0.0, 1.0))\n",
    "    return flags\n",
    "\n",
    "def bicycle_semantics(box, water_top_y):\n",
    "    x1,y1,x2,y2 = map(int, box)\n",
    "    bh = max(1, y2 - y1)\n",
    "    wheel = y1 + int(0.90*bh); seat = y1 + int(0.60*bh); handle = y1 + int(0.30*bh)\n",
    "    flags = {\n",
    "        \"bicycle_sub_wheel\": int(water_top_y > wheel),\n",
    "        \"bicycle_sub_seat\": int(water_top_y > seat),\n",
    "        \"bicycle_sub_handle\": int(water_top_y > handle)\n",
    "    }\n",
    "    denom = handle - wheel\n",
    "    if denom <= 0:\n",
    "        flags[\"bicycle_depth_norm\"] = 0.0\n",
    "    else:\n",
    "        flags[\"bicycle_depth_norm\"] = float(np.clip((water_top_y - wheel)/denom, 0.0, 1.0))\n",
    "    return flags\n",
    "\n",
    "def generic_semantics(subratio):\n",
    "    return {\n",
    "        \"obj_sub_20pct\": int(subratio > 0.20),\n",
    "        \"obj_sub_50pct\": int(subratio > 0.50),\n",
    "        \"obj_sub_80pct\": int(subratio > 0.80),\n",
    "    }\n",
    "\n",
    "def estimate_depth_cm(subratio, cname):\n",
    "    cname = safe_name(cname)\n",
    "    ref = CLASS_HEIGHT_PRIORS_CM.get(cname, DEFAULT_CLASS_HEIGHT)\n",
    "    return float(ref * subratio), float(ref)\n",
    "\n",
    "# -----------------------\n",
    "# Assemble single GT row\n",
    "# -----------------------\n",
    "def process_gt_box(rgb, box, gt_level, cname_from_csv, poses_arr, mask, img_w, img_h, obj_idx):\n",
    "    x1,y1,x2,y2 = map(int, box)\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return None\n",
    "    base = compute_base_features([x1,y1,x2,y2], mask, img_w, img_h)\n",
    "    pose_for_obj = nearest_pose([x1,y1,x2,y2], poses_arr) if len(poses_arr) > 0 else None\n",
    "    cname = safe_name(cname_from_csv)\n",
    "    sem = {}\n",
    "    if cname == \"person\":\n",
    "        sem.update(person_semantics(pose_for_obj, base[\"water_top_y\"], [x1,y1,x2,y2]))\n",
    "    elif cname == \"car\":\n",
    "        sem.update(car_semantics([x1,y1,x2,y2], base[\"water_top_y\"]))\n",
    "    elif cname == \"bus\":\n",
    "        sem.update(bus_semantics([x1,y1,x2,y2], base[\"water_top_y\"]))\n",
    "    elif cname == \"truck\":\n",
    "        sem.update(truck_semantics([x1,y1,x2,y2], base[\"water_top_y\"]))\n",
    "    elif cname == \"motorcycle\":\n",
    "        sem.update(motorcycle_semantics([x1,y1,x2,y2], base[\"water_top_y\"]))\n",
    "    elif cname == \"bicycle\":\n",
    "        sem.update(bicycle_semantics([x1,y1,x2,y2], base[\"water_top_y\"]))\n",
    "    sem.update(generic_semantics(base[\"submergence_ratio\"]))\n",
    "    est_cm, ref_cm = estimate_depth_cm(base[\"submergence_ratio\"], cname)\n",
    "    row = {\n",
    "        \"image_path\": None,\n",
    "        \"object_name\": cname,\n",
    "        \"flood_level\": int(gt_level),\n",
    "        \"obj_id\": int(obj_idx)\n",
    "    }\n",
    "    for k,v in base.items():\n",
    "        row[k] = float(v) if isinstance(v, (int, float, np.floating, np.integer)) else v\n",
    "    row[\"estimated_depth_cm\"] = float(est_cm)\n",
    "    row[\"ref_height_cm\"] = float(ref_cm)\n",
    "    row[\"physics_residual\"] = 0.0\n",
    "    for f in ALL_FEATURES:\n",
    "        if f not in row:\n",
    "            row[f] = 0.0\n",
    "    for k,v in sem.items():\n",
    "        row[k] = float(v)\n",
    "    for k in ALL_FEATURES:\n",
    "        if isinstance(row.get(k, None), (int, np.integer)):\n",
    "            row[k] = float(row[k])\n",
    "    return row\n",
    "\n",
    "# -----------------------\n",
    "# Cleaning utility for final df\n",
    "# -----------------------\n",
    "def clean_df(df):\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "# -----------------------\n",
    "# MAIN: read filtered CSV and compute features\n",
    "# -----------------------\n",
    "def main():\n",
    "    if not os.path.exists(IN_FILTERED_CSV):\n",
    "        print(\"Filtered CSV not found:\", IN_FILTERED_CSV)\n",
    "        return\n",
    "\n",
    "    df_filtered = pd.read_csv(IN_FILTERED_CSV)\n",
    "    print(\"Filtered rows:\", len(df_filtered))\n",
    "\n",
    "    # build assignment map: (image_path, flood_level) -> list of object_name in original order\n",
    "    assign_map = {}\n",
    "    for _, r in df_filtered.iterrows():\n",
    "        ip = r[\"image_path\"]\n",
    "        # safer parsing if flood_level is string/float\n",
    "        try:\n",
    "            fl = int(float(r[\"flood_level\"]))\n",
    "        except Exception:\n",
    "            fl = int(r[\"flood_level\"])\n",
    "        key = (ip, fl)\n",
    "        assign_map.setdefault(key, []).append(r[\"object_name\"])\n",
    "\n",
    "    need_counts = {k: len(v) for k,v in assign_map.items()}\n",
    "    print(\"Unique (image,level) groups:\", len(need_counts))\n",
    "\n",
    "    # load models\n",
    "    print(\"Loading pose & waterseg models (if available)...\")\n",
    "    pose_model, wseg_model = load_models()\n",
    "    if pose_model is None:\n",
    "        print(\"NOTE: Pose model not found; pose features will be skipped.\")\n",
    "    if wseg_model is None:\n",
    "        print(\"NOTE: Water segmentation model not found; water masks will be zeros.\")\n",
    "\n",
    "    out_rows = []\n",
    "    processed_counts = {}\n",
    "\n",
    "    unique_images = sorted(df_filtered[\"image_path\"].unique())\n",
    "    print(\"Images to process:\", len(unique_images))\n",
    "\n",
    "    for img_path in tqdm(unique_images):\n",
    "        img_name = Path(img_path).name\n",
    "        # find label file\n",
    "        lbl_path = None\n",
    "        for d in LBL_DIRS.values():\n",
    "            candidate = os.path.join(d, Path(img_name).stem + \".txt\")\n",
    "            if os.path.exists(candidate):\n",
    "                lbl_path = candidate\n",
    "                break\n",
    "        if lbl_path is None:\n",
    "            # no label file found — skip\n",
    "            print(\"Label missing for:\", img_path)\n",
    "            continue\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(\"Failed to load image:\", img_path)\n",
    "            continue\n",
    "        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h_img, w_img = rgb.shape[:2]\n",
    "\n",
    "        # read GT boxes in label file\n",
    "        with open(lbl_path, \"r\") as f:\n",
    "            lines = [ln.strip() for ln in f.read().splitlines() if ln.strip()]\n",
    "\n",
    "        gt_boxes = []\n",
    "        for ln in lines:\n",
    "            parts = ln.split()\n",
    "            if len(parts) != 5:\n",
    "                continue\n",
    "            # map label-index via data.yaml mapping if available\n",
    "            try:\n",
    "                lab_idx = int(parts[0])\n",
    "                cls_id = int(idx_to_level.get(lab_idx, lab_idx))\n",
    "            except Exception:\n",
    "                # fallback: try direct int cast (original behaviour)\n",
    "                try:\n",
    "                    cls_id = int(parts[0])\n",
    "                except:\n",
    "                    continue\n",
    "            cx = float(parts[1]) * w_img; cy = float(parts[2]) * h_img\n",
    "            bw = float(parts[3]) * w_img; bh = float(parts[4]) * h_img\n",
    "            x1 = int(cx - bw/2); y1 = int(cy - bh/2); x2 = int(cx + bw/2); y2 = int(cy + bh/2)\n",
    "            x1 = max(0, x1); y1 = max(0, y1); x2 = min(w_img-1, x2); y2 = min(h_img-1, y2)\n",
    "            if x2 <= x1 or y2 <= y1:\n",
    "                continue\n",
    "            gt_boxes.append((x1,y1,x2,y2,cls_id))\n",
    "\n",
    "        if len(gt_boxes) == 0:\n",
    "            continue\n",
    "\n",
    "        # compute image-level masks and poses once\n",
    "        mask = run_water_seg(wseg_model, rgb) if wseg_model is not None else np.zeros((h_img, w_img), dtype=np.uint8)\n",
    "        poses = []\n",
    "        if pose_model is not None:\n",
    "            try:\n",
    "                pres = pose_model.predict(rgb, imgsz=640, conf=POSE_CONF_TH, verbose=False)[0]\n",
    "                if getattr(pres, \"keypoints\", None) is not None:\n",
    "                    arr = pres.keypoints.data.cpu().numpy()\n",
    "                    for i in range(arr.shape[0]):\n",
    "                        poses.append(arr[i])\n",
    "            except Exception:\n",
    "                poses = []\n",
    "\n",
    "        # iterate GT boxes in file order, and assign only if requested by assign_map\n",
    "        for idx, (x1,y1,x2,y2,cls_id) in enumerate(gt_boxes):\n",
    "            key = (img_path, int(cls_id))\n",
    "            needed = need_counts.get(key, 0)\n",
    "            already = processed_counts.get(key, 0)\n",
    "            if already >= needed:\n",
    "                continue\n",
    "            name_list = assign_map.get(key, [])\n",
    "            if len(name_list) == 0:\n",
    "                # nothing to assign\n",
    "                continue\n",
    "            cname = name_list.pop(0)\n",
    "            cname_norm = safe_name(cname)\n",
    "            row = process_gt_box(rgb, (x1,y1,x2,y2), cls_id, cname_norm, poses, mask, w_img, h_img, already)\n",
    "            if row is None:\n",
    "                continue\n",
    "            row[\"image_path\"] = img_path\n",
    "            row[\"object_name\"] = cname_norm\n",
    "            out_rows.append(row)\n",
    "            processed_counts[key] = processed_counts.get(key, 0) + 1\n",
    "\n",
    "    print(\"Feature rows produced:\", len(out_rows))\n",
    "\n",
    "    # check unmet groups\n",
    "    unmet = []\n",
    "    for k, need in need_counts.items():\n",
    "        got = processed_counts.get(k, 0)\n",
    "        if got < need:\n",
    "            unmet.append((k, need, got))\n",
    "    if len(unmet) > 0:\n",
    "        print(\"WARNING: some (image,level) groups had fewer GT boxes than rows requested in filtered CSV:\")\n",
    "        for (ip, lvl), need, got in unmet:\n",
    "            print(f\"  {ip} | level={lvl} -> needed {need}, got {got}\")\n",
    "\n",
    "    if len(out_rows) == 0:\n",
    "        print(\"No feature rows produced. Exiting.\")\n",
    "        return\n",
    "\n",
    "    df_out = pd.DataFrame(out_rows)\n",
    "    df_out = clean_df(df_out)\n",
    "\n",
    "    # ensure columns order: image_path, object_name, flood_level, ALL_FEATURES...\n",
    "    cols = [\"image_path\", \"object_name\", \"flood_level\"] + ALL_FEATURES\n",
    "    # keep only columns present in df_out (defensive)\n",
    "    cols = [c for c in cols if c in df_out.columns]\n",
    "    df_out = df_out[cols]\n",
    "\n",
    "    df_out.to_csv(OUT_CSV, index=False)\n",
    "    print(\"Saved final CSV:\", OUT_CSV)\n",
    "    print(\"Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc72acb-674f-4fe2-ad16-9cb4f365c3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = \"/home/arnab/Desktop/yolo/data/Flood_model/yolov11_refined_balanced/object_label_csv2/object_and_labels_final_44feat.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(\"\\n===== UNIQUE CLASSES =====\")\n",
    "print(df[\"object_name\"].unique())\n",
    "\n",
    "print(\"\\n===== UNIQUE FLOOD LEVELS =====\")\n",
    "print(sorted(df[\"flood_level\"].unique()))\n",
    "\n",
    "print(\"\\n===== CSV HEAD (FIRST 5 ROWS) =====\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n===== COLUMN NAMES =====\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc32f4fb-89b7-46e8-b6dd-79e4a4d2c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = \"/home/arnab/Desktop/yolo/data/Flood_model/yolov11_refined_balanced/object_label_csv2/object_and_labels_final_44feat.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(\"Flood level counts (0–10):\\n\")\n",
    "counts = df[\"flood_level\"].value_counts().sort_index()\n",
    "\n",
    "for level, count in counts.items():\n",
    "    print(f\"{level} = {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a821c328-bb4d-4eeb-a423-cb383db533ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "CSV_IN = \"/home/arnab/Desktop/yolo/data/Flood_model/yolov11_refined_balanced/object_label_csv2/object_and_labels_final_44feat.csv\"\n",
    "\n",
    "OUT_TRAIN = \"/home/arnab/Desktop/yolo/data/Flood_model/yolov11_refined_balanced/object_label_csv2/train_balanced_80.csv\"\n",
    "OUT_TEST  = \"/home/arnab/Desktop/yolo/data/Flood_model/yolov11_refined_balanced/object_label_csv2/test_balanced_20.csv\"\n",
    "\n",
    "df = pd.read_csv(CSV_IN)\n",
    "\n",
    "print(\"Original counts:\")\n",
    "print(df[\"flood_level\"].value_counts().sort_index())\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 1. DETERMINE MAX COUNT (for oversampling target)\n",
    "# ------------------------------------------------------\n",
    "level_counts = df[\"flood_level\"].value_counts()\n",
    "max_count = level_counts.max()\n",
    "\n",
    "print(\"\\nMax samples among levels =\", max_count)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 2. OVERSAMPLE EACH LEVEL TO max_count\n",
    "# ------------------------------------------------------\n",
    "balanced_parts = []\n",
    "\n",
    "for level in sorted(df[\"flood_level\"].unique()):\n",
    "    sub = df[df[\"flood_level\"] == level]\n",
    "    if len(sub) < max_count:\n",
    "        # oversample with replacement\n",
    "        sub_over = sub.sample(max_count, replace=True, random_state=42)\n",
    "    else:\n",
    "        sub_over = sub\n",
    "    balanced_parts.append(sub_over)\n",
    "\n",
    "df_balanced = pd.concat(balanced_parts, ignore_index=True)\n",
    "\n",
    "print(\"\\nAfter oversampling:\")\n",
    "print(df_balanced[\"flood_level\"].value_counts().sort_index())\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 3. STRATIFIED TRAIN/TEST SPLIT\n",
    "# ------------------------------------------------------\n",
    "train_df, test_df = train_test_split(\n",
    "    df_balanced,\n",
    "    test_size=0.10,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    stratify=df_balanced[\"flood_level\"]\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 4. SAVE OUTPUT CSVs\n",
    "# ------------------------------------------------------\n",
    "train_df.to_csv(OUT_TRAIN, index=False)\n",
    "test_df.to_csv(OUT_TEST, index=False)\n",
    "\n",
    "print(\"\\nSaved train:\", OUT_TRAIN, \"rows:\", len(train_df))\n",
    "print(\"Saved test:\", OUT_TEST, \"rows:\", len(test_df))\n",
    "\n",
    "print(\"\\nTRAIN distribution:\")\n",
    "print(train_df[\"flood_level\"].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nTEST distribution:\")\n",
    "print(test_df[\"flood_level\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd734f2-ada7-4176-993e-686c398e56d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import os\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "TRAIN_IN = \"/home/arnab/Desktop/yolo/data/Flood_model/yolov11_refined_balanced/object_label_csv2/train_balanced_80.csv\"\n",
    "OUT_DIR = \"/home/arnab/Desktop/yolo/data/Flood_model/yolov11_refined_balanced/object_label_csv2/kfolds_train_only\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "K = 5  # change if needed (e.g., 5, 10, etc.)\n",
    "\n",
    "# -----------------------------\n",
    "# LOAD TRAIN CSV\n",
    "# -----------------------------\n",
    "df = pd.read_csv(TRAIN_IN)\n",
    "\n",
    "print(\"Balanced TRAIN level counts:\")\n",
    "print(df[\"flood_level\"].value_counts().sort_index())\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 1. STRATIFIED K-FOLD SPLIT ON TRAIN ONLY\n",
    "# --------------------------------------------------------\n",
    "skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=42)\n",
    "\n",
    "X = df\n",
    "y = df[\"flood_level\"]\n",
    "\n",
    "fold = 1\n",
    "for train_idx, val_idx in skf.split(X, y):\n",
    "\n",
    "    df_train_k = df.iloc[train_idx].reset_index(drop=True)\n",
    "    df_val_k   = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    train_path = f\"{OUT_DIR}/kfold_train_{fold}.csv\"\n",
    "    val_path   = f\"{OUT_DIR}/kfold_val_{fold}.csv\"\n",
    "\n",
    "    df_train_k.to_csv(train_path, index=False)\n",
    "    df_val_k.to_csv(val_path, index=False)\n",
    "\n",
    "    print(f\"\\n===== FOLD {fold} =====\")\n",
    "    print(\"Train rows:\", len(df_train_k))\n",
    "    print(df_train_k[\"flood_level\"].value_counts().sort_index())\n",
    "\n",
    "    print(\"\\nVal rows:\", len(df_val_k))\n",
    "    print(df_val_k[\"flood_level\"].value_counts().sort_index())\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "print(\"\\nK-fold CSVs saved in:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbc0116-adf3-41e6-a2ab-ffd14fbe2cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 - setup\n",
    "import os, sys, math, random, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import joblib\n",
    "\n",
    "# NGBoost (replaces XGB)\n",
    "from ngboost import NGBClassifier\n",
    "from ngboost.distns import k_categorical\n",
    "from ngboost.scores import LogScore\n",
    "\n",
    "# config\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Paths (update for new 50+ features)\n",
    "DATA_ROOT = \"/home/arnab/Desktop/yolo/data/Flood_model/yolov11_refined_balanced/object_label_csv2ConvNeXT_NGBoost_FShL+llm\"\n",
    "K_FOLD_DIR = f\"{DATA_ROOT}/kfolds_train_only\"\n",
    "TEST_CSV = f\"{DATA_ROOT}/test_balanced_20.csv\"\n",
    "\n",
    "# new output folder\n",
    "OUT_DIR = f\"{DATA_ROOT}/hybrid_resnet50_xgb_fusion_v1\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# YOLO weights for inference (feature computation)\n",
    "YOLO_DET_PATH  = \"yolo11m.pt\"\n",
    "YOLO_POSE_PATH = \"yolo11m-pose.pt\"\n",
    "YOLO_WSEG_PATH = \"/home/arnab/Desktop/yolo/data/Flood_model/UrbanFlood_WaterSeg/yolo11m-water-seg/weights/best.pt\"\n",
    "\n",
    "# model/hyperparams\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "SCHEDULER = \"cosine\"   # \"cosine\",\"onecycle\",\"step\"\n",
    "NUM_CLASSES = 11\n",
    "\n",
    "ACT = nn.SiLU  # option A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76d4b80-f7e9-4d64-abe5-ea9574f3ac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 - feature list + utils\n",
    "\n",
    "ALL_FEATURES = [\n",
    "    # geometry\n",
    "    \"box_x1\",\"box_y1\",\"box_x2\",\"box_y2\",\"box_w\",\"box_h\",\"box_area\",\n",
    "\n",
    "    # water/base\n",
    "    \"water_pixels\",\"water_area_frac\",\"water_top_y\",\"water_height_frac\",\n",
    "    \"sgf_mean\",\"sgf_max\",\"WOMI\",\"submergence_ratio\",\"bbox_ratio\",\n",
    "    \"water_frac_ratio\",\"waterline_norm\",\n",
    "\n",
    "    # physics\n",
    "    \"estimated_depth_cm\",\"ref_height_cm\",\"physics_residual\",\n",
    "\n",
    "    # person semantics (renamed from human_*)\n",
    "    \"person_sub_ankle\",\"person_sub_knee\",\"person_sub_hip\",\n",
    "    \"person_sub_chest\",\"person_sub_head\",\"person_depth_norm\",\n",
    "\n",
    "    # 🔥 NEW PERSON SEMANTICS\n",
    "    \"person_sub_mid_thigh\",\n",
    "    \"person_sub_upper_thigh\",\n",
    "    \"person_sub_lower_waist\",\n",
    "    \"person_depth_fine_ratio\",\n",
    "\n",
    "    # generic submergence flags\n",
    "    \"obj_sub_20pct\",\"obj_sub_50pct\",\"obj_sub_80pct\",\n",
    "\n",
    "    # car semantics\n",
    "    \"car_sub_wheel\",\"car_sub_mid\",\"car_sub_window\",\"car_sub_roof\",\"car_depth_norm\",\n",
    "\n",
    "    # 🔥 NEW CAR SEMANTIC\n",
    "    \"car_sub_half_door\",\n",
    "\n",
    "    # bus semantics\n",
    "    \"bus_sub_wheel\",\"bus_sub_mid\",\"bus_sub_window\",\"bus_sub_roof\",\"bus_depth_norm\",\n",
    "\n",
    "    # truck semantics\n",
    "    \"truck_sub_wheel\",\"truck_sub_mid\",\"truck_sub_window\",\"truck_sub_roof\",\"truck_depth_norm\",\n",
    "\n",
    "    # 🔥 NEW TRUCK SEMANTIC\n",
    "    \"truck_sub_hubcap\",\n",
    "\n",
    "    # motorcycle semantics\n",
    "    \"motorcycle_sub_wheel\",\"motorcycle_sub_seat\",\"motorcycle_sub_handle\",\"motorcycle_depth_norm\",\n",
    "\n",
    "    # 🔥 NEW MOTORCYCLE SEMANTIC\n",
    "    \"motorcycle_sub_engine_level\",\n",
    "\n",
    "    # bicycle semantics\n",
    "    \"bicycle_sub_wheel\",\"bicycle_sub_seat\",\"bicycle_sub_handle\",\"bicycle_depth_norm\"\n",
    "]\n",
    "\n",
    "def safe_load_image_rgb(path):\n",
    "    img = cv2.imread(path)\n",
    "    if img is None:\n",
    "        try:\n",
    "            pil = Image.open(path).convert(\"RGB\")\n",
    "            arr = np.array(pil)[:, :, ::-1]\n",
    "            return arr\n",
    "        except:\n",
    "            return np.zeros((IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.uint8)\n",
    "    return img[:, :, ::-1]\n",
    "\n",
    "# image transforms\n",
    "train_transform = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.RandomResizedCrop(IMAGE_SIZE, scale=(0.8,1.0)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ColorJitter(0.2,0.2,0.2,0.02),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "val_transform = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7ec6cd-01c5-4bc0-be44-a73e0f50d501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 - dataset\n",
    "class HybridCropDataset(Dataset):\n",
    "    def __init__(self, csv_path, feature_cols, transform=None, scaler: StandardScaler=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Ensure ALL_FEATURES exist (including new ones)\n",
    "        for c in feature_cols:\n",
    "            if c not in self.df.columns:\n",
    "                self.df[c] = 0.0\n",
    "\n",
    "        self.feature_cols = feature_cols\n",
    "        self.transform = transform\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # -------- load image --------\n",
    "        img_path = row[\"image_path\"]\n",
    "        img = safe_load_image_rgb(img_path)\n",
    "        h, w = img.shape[:2]\n",
    "\n",
    "        # -------- crop by GT box --------\n",
    "        x1 = int(row[\"box_x1\"]); y1 = int(row[\"box_y1\"])\n",
    "        x2 = int(row[\"box_x2\"]); y2 = int(row[\"box_y2\"])\n",
    "\n",
    "        # boundary clamp\n",
    "        x1 = max(0, min(x1, w-1)); x2 = max(0, min(x2, w-1))\n",
    "        y1 = max(0, min(y1, h-1)); y2 = max(0, min(y2, h-1))\n",
    "\n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            crop = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "        else:\n",
    "            crop = cv2.resize(img[y1:y2, x1:x2], (IMAGE_SIZE, IMAGE_SIZE))\n",
    "\n",
    "        # -------- transforms --------\n",
    "        if self.transform is not None:\n",
    "            img_t = self.transform(crop)\n",
    "        else:\n",
    "            img_t = val_transform(crop)\n",
    "\n",
    "        # -------- load tabular features --------\n",
    "        feats = np.array(\n",
    "            [\n",
    "                float(row[c]) if not pd.isna(row[c]) else 0.0\n",
    "                for c in self.feature_cols\n",
    "            ],\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # scaling (safer reshape)\n",
    "        if self.scaler is not None:\n",
    "            feats = self.scaler.transform(feats.reshape(1, -1))[0]\n",
    "\n",
    "        feats_t = torch.from_numpy(feats.astype(np.float32))\n",
    "\n",
    "        # -------- label --------\n",
    "        label = int(row[\"flood_level\"])\n",
    "\n",
    "        return img_t, feats_t, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5fc81d-c521-47ec-88aa-b41fe6ccda2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 - models (ConvNeXt backbone + FusionMLP)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Try to import torchvision convnext variants and weights API; fallback if not present\n",
    "try:\n",
    "    from torchvision.models import convnext_tiny, convnext_small, convnext_base, convnext_large\n",
    "    try:\n",
    "        # modern weights API names (if torchvision supports)\n",
    "        from torchvision.models import ConvNeXt_Tiny_Weights, ConvNeXt_Small_Weights, ConvNeXt_Base_Weights, ConvNeXt_Large_Weights\n",
    "        _TV_HAS_WEIGHTS = True\n",
    "    except Exception:\n",
    "        _TV_HAS_WEIGHTS = False\n",
    "except Exception:\n",
    "    # torchvision may be older; we'll try to import via timm if available\n",
    "    _TV_HAS_WEIGHTS = False\n",
    "    convnext_tiny = convnext_small = convnext_base = convnext_large = None\n",
    "\n",
    "class ConvNeXtBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "    ConvNeXt backbone wrapper.\n",
    "    - variant: 'convnext_tiny','convnext_small','convnext_base','convnext_large'\n",
    "    - removes classifier and returns pooled embedding vector per image\n",
    "    - infers embedding dim automatically via a dummy forward\n",
    "    \"\"\"\n",
    "    def __init__(self, variant=\"convnext_base\", pretrained=True, img_size=IMAGE_SIZE, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        variant = variant.lower()\n",
    "        supported = {\n",
    "            \"convnext_tiny\": convnext_tiny,\n",
    "            \"convnext_small\": convnext_small,\n",
    "            \"convnext_base\": convnext_base,\n",
    "            \"convnext_large\": convnext_large\n",
    "        }\n",
    "\n",
    "        model = None\n",
    "        # 1) Try torchvision provided constructors first\n",
    "        if variant in supported and supported[variant] is not None:\n",
    "            ctor = supported[variant]\n",
    "            if _TV_HAS_WEIGHTS and pretrained:\n",
    "                # map to weights constant if available\n",
    "                try:\n",
    "                    weights_map = {\n",
    "                        \"convnext_tiny\": ConvNeXt_Tiny_Weights.IMAGENET1K_V1,\n",
    "                        \"convnext_small\": ConvNeXt_Small_Weights.IMAGENET1K_V1,\n",
    "                        \"convnext_base\": ConvNeXt_Base_Weights.IMAGENET1K_V1,\n",
    "                        \"convnext_large\": ConvNeXt_Large_Weights.IMAGENET1K_V1\n",
    "                    }\n",
    "                    w = weights_map.get(variant, None)\n",
    "                    if w is not None:\n",
    "                        model = ctor(weights=w)\n",
    "                    else:\n",
    "                        model = ctor(pretrained=pretrained)\n",
    "                except Exception:\n",
    "                    # fallback\n",
    "                    model = ctor(pretrained=pretrained)\n",
    "            else:\n",
    "                try:\n",
    "                    model = ctor(pretrained=pretrained)\n",
    "                except Exception:\n",
    "                    model = ctor()\n",
    "        # 2) If torchvision ctor not present or failed, try timm\n",
    "        if model is None:\n",
    "            try:\n",
    "                import timm\n",
    "                # map to common timm names\n",
    "                timm_names = {\n",
    "                    \"convnext_tiny\": \"convnext_tiny\",\n",
    "                    \"convnext_small\": \"convnext_small\",\n",
    "                    \"convnext_base\": \"convnext_base\",\n",
    "                    \"convnext_large\": \"convnext_large\"\n",
    "                }\n",
    "                name = timm_names.get(variant, \"convnext_base\")\n",
    "                model = timm.create_model(name, pretrained=pretrained)\n",
    "                print(f\"[ConvNeXtBackbone] loaded timm model {name}\")\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to create ConvNeXt variant '{variant}' from torchvision or timm. Error: {e}\")\n",
    "\n",
    "        # remove classifier head robustly\n",
    "        removed = False\n",
    "        if hasattr(model, \"classifier\"):\n",
    "            try:\n",
    "                model.classifier = nn.Identity(); removed = True\n",
    "            except Exception:\n",
    "                pass\n",
    "        if not removed:\n",
    "            if hasattr(model, \"head\"):\n",
    "                model.head = nn.Identity(); removed = True\n",
    "            elif hasattr(model, \"fc\"):\n",
    "                model.fc = nn.Identity(); removed = True\n",
    "            elif hasattr(model, \"reset_classifier\"):\n",
    "                try:\n",
    "                    model.reset_classifier(0)\n",
    "                    removed = True\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        self.model = model\n",
    "        self._infer_out_dim()\n",
    "\n",
    "    def _infer_out_dim(self):\n",
    "        self.model.eval()\n",
    "        dummy = torch.randn(1, 3, self.img_size, self.img_size)\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                # prefer forward_features if available\n",
    "                if hasattr(self.model, \"forward_features\"):\n",
    "                    out = self.model.forward_features(dummy)\n",
    "                else:\n",
    "                    out = self.model(dummy)\n",
    "        except Exception:\n",
    "            # attempt using features + avgpool if available\n",
    "            if hasattr(self.model, \"features\"):\n",
    "                with torch.no_grad():\n",
    "                    feats = self.model.features(dummy)\n",
    "                    if hasattr(self.model, \"avgpool\"):\n",
    "                        feats = self.model.avgpool(feats)\n",
    "                    out = feats\n",
    "            else:\n",
    "                raise RuntimeError(\"Unable to run dummy forward on ConvNeXt model to infer embedding dim.\")\n",
    "\n",
    "        if isinstance(out, (list, tuple)):\n",
    "            out = out[0]\n",
    "        if out.dim() == 4:\n",
    "            out = torch.nn.functional.adaptive_avg_pool2d(out, 1).reshape(out.shape[0], -1)\n",
    "\n",
    "        if not isinstance(out, torch.Tensor):\n",
    "            raise RuntimeError(\"Unexpected output type from ConvNeXt forward: \" + str(type(out)))\n",
    "\n",
    "        self.out_dim = out.shape[1]\n",
    "        print(f\"[ConvNeXtBackbone] inferred out_dim = {self.out_dim}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = None\n",
    "        # prefer forward_features if present\n",
    "        if hasattr(self.model, \"forward_features\"):\n",
    "            out = self.model.forward_features(x)\n",
    "        else:\n",
    "            out = self.model(x)\n",
    "        if isinstance(out, (list, tuple)):\n",
    "            out = out[0]\n",
    "        if out.dim() == 4:\n",
    "            out = torch.nn.functional.adaptive_avg_pool2d(out, 1).reshape(out.shape[0], -1)\n",
    "        return out\n",
    "\n",
    "\n",
    "# ---- FusionMLP (unchanged from your original) ----\n",
    "class FusionMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Hybrid head: concat(emb, XGB_probs) → MLP → logits\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 emb_dim=2048,\n",
    "                 xgb_prob_dim=NUM_CLASSES,\n",
    "                 hidden=512,\n",
    "                 out_dim=NUM_CLASSES,\n",
    "                 use_ordinal_branch=False):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_ordinal = use_ordinal_branch\n",
    "\n",
    "        # small normalization helps fusion stability\n",
    "        self.fuse_norm = nn.LayerNorm(emb_dim + xgb_prob_dim)\n",
    "\n",
    "        # main fusion network\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim + xgb_prob_dim, hidden),\n",
    "            ACT(),\n",
    "            nn.BatchNorm1d(hidden),\n",
    "            nn.Dropout(0.30),\n",
    "\n",
    "            nn.Linear(hidden, hidden // 2),\n",
    "            ACT(),\n",
    "            nn.BatchNorm1d(hidden // 2),\n",
    "            nn.Dropout(0.20),\n",
    "\n",
    "            nn.Linear(hidden // 2, out_dim)\n",
    "        )\n",
    "\n",
    "        if self.use_ordinal:\n",
    "            self.ordinal_head = nn.Sequential(\n",
    "                nn.Linear(hidden // 2, (NUM_CLASSES - 1)),\n",
    "            )\n",
    "        else:\n",
    "            self.ordinal_head = None\n",
    "\n",
    "    def forward(self, emb, xgb_probs, return_features=False):\n",
    "        x = torch.cat([emb, xgb_probs], dim=1)\n",
    "        x = self.fuse_norm(x)\n",
    "        out = self.mlp(x)\n",
    "\n",
    "        if return_features:\n",
    "            return x, out\n",
    "\n",
    "        if self.use_ordinal:\n",
    "            hidden_proj = self.mlp[:-1](x)\n",
    "            ord_out = self.ordinal_head(hidden_proj)\n",
    "            return out, ord_out\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001fee0f-e32d-43cb-bf32-1e9767c9db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell5\n",
    "\n",
    "def extract_embeddings(backbone, loader, device=DEVICE, return_paths=False):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      embs: np.array (N, D)\n",
    "      labels: np.array (N,)\n",
    "      paths: list (N,) optional - image paths in same order as embeddings\n",
    "    \"\"\"\n",
    "    backbone.eval()\n",
    "    backbone.to(device)\n",
    "\n",
    "    all_embs = []\n",
    "    all_labels = []\n",
    "    all_paths = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # loader yields (imgs, feats, labels) from HybridCropDataset\n",
    "            imgs, feats, labs = batch\n",
    "            imgs = imgs.to(device)\n",
    "            emb = backbone(imgs)\n",
    "            if emb.dim() == 4:\n",
    "                emb = torch.nn.functional.adaptive_avg_pool2d(emb, 1).reshape(emb.shape[0], -1)\n",
    "\n",
    "            all_embs.append(emb.cpu().numpy().astype(np.float32))\n",
    "            all_labels.extend(labs.numpy().astype(np.int64).tolist())\n",
    "\n",
    "            # try to collect paths if dataset exposes them via attribute `df` + index\n",
    "            if hasattr(loader.dataset, \"df\"):\n",
    "                # compute the slice of indices for this batch - best-effort: rely on DataLoader order\n",
    "                # NOTE: this works if loader.shuffle=False; in train use shuffle=True so don't rely there\n",
    "                pass\n",
    "\n",
    "    embs = np.vstack(all_embs).astype(np.float32)\n",
    "    labels = np.array(all_labels, dtype=np.int64)\n",
    "\n",
    "    print(f\"Extracted embeddings: shape={embs.shape}, dtype={embs.dtype}\")\n",
    "    if return_paths:\n",
    "        # best-effort: if dataset has df and no shuffling, return df.image_path in order\n",
    "        try:\n",
    "            paths = loader.dataset.df[\"image_path\"].values.tolist()\n",
    "        except Exception:\n",
    "            paths = [None] * len(labels)\n",
    "        return embs, labels, paths\n",
    "\n",
    "    return embs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6750e79c-a8a1-4faa-b197-de18edb2a350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 — UPDATED train_kfold_hybrid (Ordinal + Focal + Label Smooth) with NGBoost\n",
    "\n",
    "# ===== Extra Losses =====\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\" Standard Focal Loss for multi-class classification \"\"\"\n",
    "    def __init__(self, gamma=2.0, weight=None):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        ce = nn.CrossEntropyLoss(weight=self.weight, reduction=\"none\")(logits, target)\n",
    "        pt = torch.exp(-ce)\n",
    "        focal = ((1 - pt) ** self.gamma) * ce\n",
    "        return focal.mean()\n",
    "\n",
    "\n",
    "class OrdinalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Penalizes large distance between prediction and true ordinal label.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=11):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        # fixed positions 0–10\n",
    "        self.positions = torch.arange(num_classes).float().to(DEVICE)\n",
    "\n",
    "    def forward(self, probs, target):\n",
    "        # probs = softmax(logits)\n",
    "        exp_val = (probs * self.positions).sum(dim=1)   # predicted numeric level\n",
    "        return ((exp_val - target.float()) ** 2).mean()\n",
    "\n",
    "\n",
    "def smooth_ce_loss(logits, target, smoothing=0.1):\n",
    "    \"\"\"\n",
    "    Label smoothing CE loss.\n",
    "    \"\"\"\n",
    "    num_classes = logits.size(1)\n",
    "    true_dist = torch.zeros_like(logits)\n",
    "    true_dist.fill_(smoothing / (num_classes - 1))\n",
    "    true_dist.scatter_(1, target.unsqueeze(1), 1 - smoothing)\n",
    "    return torch.mean(torch.sum(-true_dist * torch.log_softmax(logits, dim=1), dim=1))\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# ================ TRAIN K-FOLD ========================\n",
    "# =====================================================\n",
    "\n",
    "def train_kfold_hybrid():\n",
    "    import joblib\n",
    "\n",
    "    train_files = sorted(Path(K_FOLD_DIR).glob(\"kfold_train_*.csv\"))\n",
    "    val_files   = sorted(Path(K_FOLD_DIR).glob(\"kfold_val_*.csv\"))\n",
    "    assert len(train_files) == len(val_files) and len(train_files) > 0, \"K-fold CSVs missing\"\n",
    "\n",
    "    fold_ckpts = []\n",
    "\n",
    "    backbone = ConvNeXtBackbone(variant=\"convnext_base\", pretrained=True, img_size=IMAGE_SIZE).to(DEVICE)\n",
    "    backbone.eval()  # frozen backbone\n",
    "\n",
    "    for fold, (tr_csv, val_csv) in enumerate(zip(train_files, val_files), start=1):\n",
    "        print(f\"\\n=== FOLD {fold} ===\")\n",
    "\n",
    "        df_tr = pd.read_csv(tr_csv)\n",
    "        df_val = pd.read_csv(val_csv)\n",
    "\n",
    "        # ensure missing features exist\n",
    "        for c in ALL_FEATURES:\n",
    "            if c not in df_tr.columns: df_tr[c] = 0.0\n",
    "            if c not in df_val.columns: df_val[c] = 0.0\n",
    "\n",
    "        # fit scaler on train\n",
    "        scaler = StandardScaler()\n",
    "        X_tr = df_tr[ALL_FEATURES].fillna(0.0).values.astype(np.float32)\n",
    "        scaler.fit(X_tr)\n",
    "        # scaled train features (we will train NGBoost on these)\n",
    "        X_tr_scaled = scaler.transform(X_tr)\n",
    "\n",
    "        y_tr_np = df_tr[\"flood_level\"].values.astype(np.int32)\n",
    "\n",
    "        # datasets\n",
    "        train_ds = HybridCropDataset(str(tr_csv), ALL_FEATURES, transform=train_transform, scaler=scaler)\n",
    "        val_ds   = HybridCropDataset(str(val_csv), ALL_FEATURES, transform=val_transform, scaler=scaler)\n",
    "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "        val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "        # ---- NGBoost TRAIN (replaces XGB) -------------------\n",
    "        # Explicit multi-class categorical Dist with NUM_CLASSES categories\n",
    "        ngb_model = NGBClassifier(\n",
    "            Dist=k_categorical(NUM_CLASSES),   # 11-class categorical\n",
    "            Score=LogScore,\n",
    "            n_estimators=400,\n",
    "            learning_rate=0.05,\n",
    "            natural_gradient=True,\n",
    "            random_state=SEED,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        print(\"  [NGBoost] fitting on scaled tabular features...\")\n",
    "        ngb_model.fit(X_tr_scaled, y_tr_np)\n",
    "\n",
    "        fold_dir = Path(OUT_DIR) / f\"fold{fold}\"\n",
    "        fold_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        ngb_path = fold_dir / \"ngb_model.pkl\"\n",
    "        joblib.dump(ngb_model, ngb_path)\n",
    "\n",
    "        # ---- EXTRACT EMBEDDINGS ----------\n",
    "        emb_tr, y_tr = extract_embeddings(backbone, train_loader)\n",
    "        emb_val, y_val = extract_embeddings(backbone, val_loader)\n",
    "        # ---------- PROTOTYPE (few-shot) COMPUTE & SAVE ----------\n",
    "        \n",
    "        # Normalize embeddings per-sample\n",
    "        emb_norm = emb_tr.copy()\n",
    "        emb_norm /= (np.linalg.norm(emb_norm, axis=1, keepdims=True) + 1e-12)\n",
    "        \n",
    "        prototypes = {}\n",
    "        intra_dists = {}\n",
    "        \n",
    "        for cls in range(NUM_CLASSES):\n",
    "            idxs = np.where(y_tr == cls)[0]\n",
    "            if len(idxs) == 0:\n",
    "                prototypes[cls] = np.zeros((emb_norm.shape[1],), dtype=np.float32)\n",
    "                intra_dists[cls] = np.array([0.0], dtype=np.float32)\n",
    "                continue\n",
    "            cls_embs = emb_norm[idxs]\n",
    "            proto = cls_embs.mean(axis=0)\n",
    "            proto = proto / (np.linalg.norm(proto) + 1e-12)\n",
    "            prototypes[cls] = proto.astype(np.float32)\n",
    "        \n",
    "            # distances (cosine -> convert to 1 - cosine)\n",
    "            sims = cls_embs.dot(proto)\n",
    "            dists = 1.0 - sims\n",
    "            intra_dists[cls] = dists.astype(np.float32)\n",
    "        \n",
    "        # per-class threshold = 95th percentile of intra-class distances\n",
    "        d_thresh = {cls: float(np.percentile(intra_dists[cls], 95)) if len(intra_dists[cls])>0 else 1.0\n",
    "                   for cls in range(NUM_CLASSES)}\n",
    "        \n",
    "        # NGBoost val uncertainty proxy: use entropy of val predict_proba\n",
    "        X_val_scaled = scaler.transform(df_val[ALL_FEATURES].fillna(0.0).values.astype(np.float32))\n",
    "        val_probs = ngb_model.predict_proba(X_val_scaled)\n",
    "        # entropy per sample\n",
    "        ent = -np.sum(np.clip(val_probs, 1e-12, 1.0) * np.log(np.clip(val_probs, 1e-12, 1.0)), axis=1)\n",
    "        v_thresh = float(np.percentile(ent, 90))  # 90th percentile as threshold\n",
    "        \n",
    "        # save\n",
    "        proto_path = fold_dir / f\"prototypes_fold{fold}.npz\"\n",
    "        np.savez_compressed(\n",
    "            proto_path,\n",
    "            prototypes=prototypes,\n",
    "            d_thresh=d_thresh,\n",
    "            v_thresh=v_thresh\n",
    "        )\n",
    "        print(f\"Saved prototypes & thresholds to {proto_path}\")\n",
    "        # ---------- end prototype save ----------\n",
    "\n",
    "\n",
    "        # ---- NGBoost PROBS --------------------\n",
    "        # train probs (for fusion train)\n",
    "        prob_tr = ngb_model.predict_proba(X_tr_scaled)\n",
    "\n",
    "        # val probs (for fusion val)\n",
    "        X_val_raw = df_val[ALL_FEATURES].fillna(0.0).values.astype(np.float32)\n",
    "        X_val_scaled = scaler.transform(X_val_raw)\n",
    "        prob_val = ngb_model.predict_proba(X_val_scaled)\n",
    "\n",
    "        # ---- Fusion dataset ---------------\n",
    "        import torch.utils.data as tud\n",
    "        train_f_ds = tud.TensorDataset(\n",
    "            torch.from_numpy(emb_tr).float(),\n",
    "            torch.from_numpy(prob_tr).float(),\n",
    "            torch.from_numpy(y_tr).long()\n",
    "        )\n",
    "        val_f_ds = tud.TensorDataset(\n",
    "            torch.from_numpy(emb_val).float(),\n",
    "            torch.from_numpy(prob_val).float(),\n",
    "            torch.from_numpy(y_val).long()\n",
    "        )\n",
    "\n",
    "        train_f_loader = DataLoader(train_f_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "        val_f_loader   = DataLoader(val_f_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "        # ---- Fusion MLP --------------------\n",
    "        fusion = FusionMLP(emb_dim=emb_tr.shape[1], xgb_prob_dim=NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "        opt = optim.RAdam(fusion.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n",
    "\n",
    "        # Loss modules\n",
    "        focal_loss_fn = FocalLoss(gamma=2.0)\n",
    "        ord_loss_fn   = OrdinalLoss(num_classes=NUM_CLASSES)\n",
    "\n",
    "        best_f1 = -1\n",
    "        fusion_ckpt = fold_dir / \"fusion_best.pth\"\n",
    "        scaler_path = fold_dir / \"scaler.pkl\"\n",
    "\n",
    "        # save scaler with joblib (SAFE)\n",
    "        joblib.dump(scaler, scaler_path)\n",
    "\n",
    "        # ================= TRAINING LOOP =================\n",
    "        for epoch in range(1, EPOCHS + 1):\n",
    "            fusion.train()\n",
    "            t_loss = 0\n",
    "            t_preds, t_trues = [], []\n",
    "\n",
    "            for eb, xb, lbl in train_f_loader:\n",
    "                eb, xb, lbl = eb.to(DEVICE), xb.to(DEVICE), lbl.to(DEVICE)\n",
    "                opt.zero_grad()\n",
    "\n",
    "                logits = fusion(eb, xb)\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "                # ---- Combined loss ----\n",
    "                ce = smooth_ce_loss(logits, lbl, smoothing=0.1)\n",
    "                focal = focal_loss_fn(logits, lbl)\n",
    "                ordl = ord_loss_fn(probs, lbl)\n",
    "\n",
    "                loss = ce + 0.25 * focal + 0.5 * ordl\n",
    "\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "                t_loss += loss.item() * eb.size(0)\n",
    "                t_preds.extend(logits.argmax(1).cpu().numpy())\n",
    "                t_trues.extend(lbl.cpu().numpy())\n",
    "                scheduler.step()\n",
    "\n",
    "            tr_acc = accuracy_score(t_trues, t_preds)\n",
    "\n",
    "            # validation\n",
    "            fusion.eval()\n",
    "            v_preds, v_trues = [], []\n",
    "            with torch.no_grad():\n",
    "                for eb, xb, lbl in val_f_loader:\n",
    "                    eb, xb = eb.to(DEVICE), xb.to(DEVICE)\n",
    "                    logits = fusion(eb, xb)\n",
    "                    v_preds.extend(logits.argmax(1).cpu().numpy())\n",
    "                    v_trues.extend(lbl.numpy())\n",
    "\n",
    "            val_acc = accuracy_score(v_trues, v_preds)\n",
    "            val_f1 = f1_score(v_trues, v_preds, average=\"weighted\")\n",
    "\n",
    "            print(f\"Fold {fold} Epoch {epoch} tr_acc={tr_acc:.4f} val_acc={val_acc:.4f} val_f1={val_f1:.4f}\")\n",
    "\n",
    "            if val_f1 > best_f1:\n",
    "                best_f1 = val_f1\n",
    "                torch.save(fusion.state_dict(), fusion_ckpt)\n",
    "\n",
    "        # store metadata\n",
    "        fold_ckpts.append({\n",
    "            \"fold\": fold,\n",
    "            \"fusion_ckpt\": str(fusion_ckpt),\n",
    "            \"ngb_model\": str(ngb_path),\n",
    "            \"scaler_path\": str(scaler_path)\n",
    "        })\n",
    "\n",
    "    # FINAL meta file (SAFE)\n",
    "    meta_path = Path(OUT_DIR) / \"folds_meta.pkl\"\n",
    "    joblib.dump(fold_ckpts, meta_path)\n",
    "    print(\"Saved fold metadata:\", meta_path)\n",
    "\n",
    "    return fold_ckpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62149233-1e23-4955-ac71-7fdd4b45dc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 — UPDATED ensemble_eval_test (conservative mid-class adjustments + meta-filter)\n",
    "def ensemble_eval_test():\n",
    "    import joblib\n",
    "    from math import ceil\n",
    "    from pathlib import Path\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "    import numpy as np\n",
    "\n",
    "    meta_path = Path(OUT_DIR) / \"folds_meta.pkl\"\n",
    "    if not meta_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing {meta_path}, run training first.\")\n",
    "\n",
    "    fold_infos = joblib.load(meta_path)\n",
    "    print(f\"Loaded {len(fold_infos)} fold metadata.\")\n",
    "\n",
    "    # ================================\n",
    "    # Load test CSV\n",
    "    # ================================\n",
    "    df_test = pd.read_csv(TEST_CSV)\n",
    "    for c in ALL_FEATURES:\n",
    "        if c not in df_test.columns:\n",
    "            df_test[c] = 0.0\n",
    "\n",
    "    # ================================\n",
    "    # Backbone for embeddings\n",
    "    # ================================\n",
    "    backbone = ConvNeXtBackbone(variant=\"convnext_base\", pretrained=True, img_size=IMAGE_SIZE).to(DEVICE)\n",
    "    backbone.eval()\n",
    "\n",
    "    # dummy scaler for dataset loading (we use per-fold scalers later)\n",
    "    scaler_dummy = StandardScaler()\n",
    "    scaler_dummy.mean_  = np.zeros(len(ALL_FEATURES))\n",
    "    scaler_dummy.scale_ = np.ones(len(ALL_FEATURES))\n",
    "\n",
    "    test_ds = HybridCropDataset(TEST_CSV, ALL_FEATURES, \n",
    "                                transform=val_transform, \n",
    "                                scaler=scaler_dummy)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, \n",
    "                             shuffle=False, num_workers=4)\n",
    "\n",
    "    # ================================\n",
    "    # Extract embeddings once\n",
    "    # ================================\n",
    "    emb_list, y_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, feats, labels in tqdm(test_loader, desc=\"Extracting embeddings\"):\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            emb = backbone(imgs)\n",
    "            if emb.dim() == 4:\n",
    "                emb = torch.nn.functional.adaptive_avg_pool2d(emb, 1).reshape(emb.shape[0], -1)\n",
    "            emb_list.append(emb.cpu().numpy())\n",
    "            y_list.extend(labels.numpy())\n",
    "\n",
    "    emb_test = np.vstack(emb_list)      # shape (N, D)\n",
    "    y_test   = np.array(y_list)         # true labels\n",
    "    X_test_raw = df_test[ALL_FEATURES].values.astype(np.float32)\n",
    "\n",
    "    N = len(df_test)\n",
    "    probs_accum = np.zeros((N, NUM_CLASSES), dtype=np.float32)\n",
    "\n",
    "    # helper: proto scoring with per-class temps (ordinal smoothing)\n",
    "    def proto_scores_for_batch_with_temps(emb_batch, prototypes, temp_per_class):\n",
    "        emb_n = emb_batch / (np.linalg.norm(emb_batch, axis=1, keepdims=True) + 1e-12)  # (B,D)\n",
    "        prot_mat = np.stack([prototypes[c] for c in range(NUM_CLASSES)], axis=0)       # (C,D)\n",
    "        sims = emb_n.dot(prot_mat.T)  # (B, C) cosine sims\n",
    "\n",
    "        # apply per-class temperature scaling (treated as multiplicative scale)\n",
    "        temps = np.array([temp_per_class.get(c, 10.0) for c in range(NUM_CLASSES)], dtype=np.float32)  # (C,)\n",
    "        sims_scaled = sims * temps[None, :]  # broadcast multiply each column by its corresponding temp\n",
    "\n",
    "        sims_scaled = sims_scaled - sims_scaled.max(axis=1, keepdims=True)\n",
    "        ex = np.exp(sims_scaled)\n",
    "        proto_probs = ex / (ex.sum(axis=1, keepdims=True) + 1e-12)\n",
    "        max_sim = sims.max(axis=1)\n",
    "        argmax = sims.argmax(axis=1)\n",
    "        return proto_probs, max_sim, argmax\n",
    "\n",
    "    # ================================\n",
    "    # Process each fold\n",
    "    # ================================\n",
    "    for idx, fo in enumerate(fold_infos, start=1):\n",
    "        print(f\"\\nProcessing fold {idx}/{len(fold_infos)} ...\")\n",
    "\n",
    "        # ---- Load scaler ----\n",
    "        scaler = joblib.load(fo[\"scaler_path\"])\n",
    "\n",
    "        # ---- NGBoost predictions ----\n",
    "        ngb_model = joblib.load(fo[\"ngb_model\"])\n",
    "        X_scaled = scaler.transform(X_test_raw)\n",
    "        ngb_probs = ngb_model.predict_proba(X_scaled)   # shape (N, NUM_CLASSES)\n",
    "\n",
    "        # ---- Load Fusion MLP ----\n",
    "        fusion = FusionMLP(\n",
    "            emb_dim=emb_test.shape[1],\n",
    "            xgb_prob_dim=NUM_CLASSES\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        state = torch.load(fo[\"fusion_ckpt\"], map_location=DEVICE)\n",
    "        fusion.load_state_dict(state)\n",
    "        fusion.eval()\n",
    "\n",
    "        # ---- Forward pass for fusion MLP outputs ----\n",
    "        fold_probs = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, N, BATCH_SIZE):\n",
    "                e = torch.from_numpy(emb_test[i:i+BATCH_SIZE]).float().to(DEVICE)\n",
    "                p = torch.from_numpy(ngb_probs[i:i+BATCH_SIZE]).float().to(DEVICE)\n",
    "\n",
    "                logits = fusion(e, p)\n",
    "                soft = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "                fold_probs.append(soft)\n",
    "        fold_probs = np.vstack(fold_probs)\n",
    "\n",
    "        # ---- Attempt to load prototypes for this fold ----\n",
    "        fo_fold = int(fo.get(\"fold\", idx))\n",
    "        candidate = Path(fo[\"fusion_ckpt\"]).parent / f\"prototypes_fold{fo_fold}.npz\"\n",
    "        prototypes = None; d_thresh = None; v_thresh = None\n",
    "        if candidate.exists():\n",
    "            P = np.load(candidate, allow_pickle=True)\n",
    "            prototypes = P[\"prototypes\"].item()\n",
    "            d_thresh = P[\"d_thresh\"].item()\n",
    "            v_thresh = float(P[\"v_thresh\"])\n",
    "            print(f\"  Loaded prototypes from {candidate}\")\n",
    "        else:\n",
    "            candidate2 = Path(fo[\"ngb_model\"]).parent / f\"prototypes_fold{fo_fold}.npz\"\n",
    "            if Path(candidate2).exists():\n",
    "                P = np.load(candidate2, allow_pickle=True)\n",
    "                prototypes = P[\"prototypes\"].item()\n",
    "                d_thresh = P[\"d_thresh\"].item()\n",
    "                v_thresh = float(P[\"v_thresh\"])\n",
    "                print(f\"  Loaded prototypes from {candidate2}\")\n",
    "\n",
    "        # ---- If prototypes exist, compute proto_probs and gating; else fall back to fusion only ----\n",
    "        if prototypes is not None:\n",
    "            # --- adjust thresholds for mid classes (mild tightening) ---\n",
    "            d_thresh_adj = {int(k): float(v) for k, v in d_thresh.items()}\n",
    "            # conservative scaling: class4 -> 0.92, class3 & 5 -> 0.95\n",
    "            for c, scale in [(4, 0.92), (3, 0.95), (5, 0.95)]:\n",
    "                if c in d_thresh_adj:\n",
    "                    d_thresh_adj[c] = d_thresh_adj[c] * scale\n",
    "\n",
    "            # --- per-class temp for ordinal smoothing (mild) ---\n",
    "            temp_per_class = {c: 10.0 for c in range(NUM_CLASSES)}\n",
    "            temp_per_class[4] = 12.0\n",
    "            temp_per_class[3] = 11.0\n",
    "            temp_per_class[5] = 11.0\n",
    "\n",
    "            # compute proto_probs, max_sim, argmax for all test samples in batches\n",
    "            proto_probs_all = []\n",
    "            max_sims_all = []\n",
    "            argmax_all = []\n",
    "            for i in range(0, N, BATCH_SIZE):\n",
    "                e_batch = emb_test[i:i+BATCH_SIZE]\n",
    "                proto_p, max_s, argm = proto_scores_for_batch_with_temps(e_batch, prototypes, temp_per_class)\n",
    "                proto_probs_all.append(proto_p)\n",
    "                max_sims_all.append(max_s)\n",
    "                argmax_all.append(argm)\n",
    "            proto_probs_all = np.vstack(proto_probs_all)\n",
    "            max_sims_all = np.concatenate(max_sims_all)\n",
    "            argmax_all = np.concatenate(argmax_all)\n",
    "\n",
    "            # NGBoost entropy as uncertainty proxy\n",
    "            ent_all = -np.sum(np.clip(ngb_probs,1e-12,1.0) * np.log(np.clip(ngb_probs,1e-12,1.0)), axis=1)\n",
    "\n",
    "            # gating: keep if (1 - max_sim) <= d_thresh_adj[class] AND ent <= v_thresh\n",
    "            keep_mask = np.zeros(N, dtype=bool)\n",
    "            for i in range(N):\n",
    "                cls = int(argmax_all[i])\n",
    "                sim = float(max_sims_all[i])\n",
    "                dist = 1.0 - sim\n",
    "                cls_thresh = float(d_thresh_adj.get(cls, 1.0))\n",
    "                if (dist <= cls_thresh) and (ent_all[i] <= v_thresh):\n",
    "                    keep_mask[i] = True\n",
    "\n",
    "            # ----- CLASS-CONDITIONAL FUSION WEIGHTS (CONSERVATIVE) -----\n",
    "            # Only class 4 uses higher proto weight; classes 3 & 5 remain baseline\n",
    "            proto_arg = proto_probs_all.argmax(axis=1)  # class index per sample from proto view\n",
    "            fusion_conf = fold_probs.max(axis=1)  # confidence of fusion\n",
    "            proto_sim = max_sims_all             # similarity to proto of chosen class\n",
    "            \n",
    "            w_proto_sample = np.full(N, 0.4, dtype=np.float32)\n",
    "            \n",
    "            # apply proto boost ONLY when fusion is uncertain AND proto is confident\n",
    "            mask = (proto_arg == 4) & (fusion_conf < 0.55) & (proto_sim > 0.40)\n",
    "            w_proto_sample[mask] = 0.55\n",
    "\n",
    "            fold_combined = (w_proto_sample[:, None] * proto_probs_all) + ((1.0 - w_proto_sample)[:, None] * fold_probs)\n",
    "\n",
    "            # for samples failing gating, reduce proto influence: prefer fusion but slightly smooth\n",
    "            fold_combined[~keep_mask] = fold_probs[~keep_mask] * 0.7 + (1.0 / NUM_CLASSES) * 0.3\n",
    "\n",
    "            fold_probs = fold_combined\n",
    "        else:\n",
    "            print(\"  Prototypes not found for this fold — using fusion-MLP probs only.\")\n",
    "\n",
    "        probs_accum += fold_probs\n",
    "\n",
    "    # ================================\n",
    "    # Average across folds\n",
    "    # ================================\n",
    "    probs_avg = probs_accum / len(fold_infos)\n",
    "\n",
    "    # --- integrate meta-classifier filter for class 4 ---\n",
    "    meta_clf_path = Path(OUT_DIR) / \"meta_clf_class4.pkl\"\n",
    "    chosen_thr = 0.75  # threshold from validation sweep\n",
    "\n",
    "    probs = probs_avg.copy()\n",
    "    preds_base = probs.argmax(axis=1)\n",
    "    fusion_prob4 = probs[:, 4]\n",
    "    fusion_conf = probs.max(axis=1)\n",
    "    ngb_entropy = -np.sum(np.clip(probs, 1e-12, 1.0) * np.log(np.clip(probs, 1e-12, 1.0)), axis=1)\n",
    "\n",
    "    # compute proto_sim4\n",
    "    proto_sim4 = np.zeros(len(probs), dtype=np.float32)\n",
    "    proto_files = []\n",
    "    for fo in fold_infos:\n",
    "        cand = Path(fo[\"fusion_ckpt\"]).parent / f\"prototypes_fold{fo['fold']}.npz\"\n",
    "        if cand.exists():\n",
    "            proto_files.append(cand)\n",
    "    if proto_files:\n",
    "        P = np.load(proto_files[0], allow_pickle=True)\n",
    "        prototypes = P[\"prototypes\"].item()\n",
    "        prot4 = np.array(prototypes[4], dtype=np.float32)\n",
    "        prot4 = prot4 / (np.linalg.norm(prot4) + 1e-12)\n",
    "        emb_n = emb_test / (np.linalg.norm(emb_test, axis=1, keepdims=True) + 1e-12)\n",
    "        proto_sim4 = emb_n.dot(prot4)\n",
    "\n",
    "    X_meta = np.vstack([fusion_prob4, fusion_conf, proto_sim4, ngb_entropy]).T\n",
    "\n",
    "    # load or train meta classifier\n",
    "    if meta_clf_path.exists():\n",
    "        meta_clf = joblib.load(meta_clf_path)\n",
    "    else:\n",
    "        X_tr, X_val, y_tr, y_val, idx_tr, idx_val = train_test_split(\n",
    "            X_meta, (y_test==4).astype(int), np.arange(len(X_meta)),\n",
    "            test_size=0.25, random_state=42, stratify=(y_test==4).astype(int)\n",
    "        )\n",
    "        meta_clf = LogisticRegression(max_iter=3000, class_weight=\"balanced\")\n",
    "        meta_clf.fit(X_tr, y_tr)\n",
    "        joblib.dump(meta_clf, meta_clf_path)\n",
    "        print(\"Trained & saved meta classifier to:\", meta_clf_path)\n",
    "\n",
    "    meta_conf_all = meta_clf.predict_proba(X_meta)[:, 1]\n",
    "    probs_proc = probs.copy()\n",
    "    demote_mask_all = (preds_base == 4) & (meta_conf_all < chosen_thr)\n",
    "    if demote_mask_all.any():\n",
    "        probs_proc[demote_mask_all, 4] = 0.0\n",
    "        row_sums = probs_proc.sum(axis=1, keepdims=True); row_sums[row_sums == 0] = 1.0\n",
    "        probs_proc = probs_proc / row_sums\n",
    "\n",
    "    preds = probs_proc.argmax(axis=1)\n",
    "\n",
    "    acc  = accuracy_score(y_test, preds)\n",
    "    f1   = f1_score(y_test, preds, average=\"weighted\")\n",
    "    prec = precision_score(y_test, preds, average=\"weighted\", zero_division=0)\n",
    "    rec  = recall_score(y_test, preds, average=\"weighted\", zero_division=0)\n",
    "\n",
    "    print(\"\\n===== FINAL ENSEMBLE RESULTS =====\")\n",
    "    print(\"Accuracy :\", acc)\n",
    "    print(\"F1 Score :\", f1)\n",
    "    print(\"Precision:\", prec)\n",
    "    print(\"Recall   :\", rec)\n",
    "\n",
    "    return {\n",
    "        \"acc\": acc, \"f1\": f1, \"precision\": prec, \"recall\": rec,\n",
    "        \"preds\": preds, \"probs\": probs_proc, \"trues\": y_test\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4355d78d-2a4c-4616-96f0-2672aa70101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 8 — Epoch-wise metrics logger & plotter\n",
    "# ================================\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "\n",
    "# Global container (persists during training)\n",
    "EPOCH_METRICS = defaultdict(list)\n",
    "\n",
    "def log_epoch_metrics(epoch, fold, y_true, y_pred, out_dir):\n",
    "    \"\"\"\n",
    "    Logs per-class metrics for a given epoch and fold.\n",
    "    \"\"\"\n",
    "    for cls in range(NUM_CLASSES):\n",
    "        mask = (y_true == cls)\n",
    "\n",
    "        acc = (y_pred[mask] == cls).mean() if mask.sum() > 0 else 0.0\n",
    "        prec = precision_score(y_true, y_pred, labels=[cls],\n",
    "                               average=\"macro\", zero_division=0)\n",
    "        rec  = recall_score(y_true, y_pred, labels=[cls],\n",
    "                            average=\"macro\", zero_division=0)\n",
    "        f1   = f1_score(y_true, y_pred, labels=[cls],\n",
    "                        average=\"macro\", zero_division=0)\n",
    "\n",
    "        EPOCH_METRICS[(fold, cls)].append({\n",
    "            \"epoch\": epoch,\n",
    "            \"accuracy\": acc,\n",
    "            \"precision\": prec,\n",
    "            \"recall\": rec,\n",
    "            \"f1\": f1\n",
    "        })\n",
    "\n",
    "    # persist after every epoch (safe)\n",
    "    save_path = Path(out_dir) / \"epoch_metrics.pkl\"\n",
    "    joblib.dump(dict(EPOCH_METRICS), save_path)\n",
    "\n",
    "\n",
    "# ================================\n",
    "# Plotting function (run AFTER training)\n",
    "# ================================\n",
    "def plot_epoch_metrics(out_dir, fold=1):\n",
    "    \"\"\"\n",
    "    Plots epoch-wise metrics for each class for a given fold.\n",
    "    \"\"\"\n",
    "    path = Path(out_dir) / \"epoch_metrics.pkl\"\n",
    "    assert path.exists(), \"epoch_metrics.pkl not found\"\n",
    "\n",
    "    data = joblib.load(path)\n",
    "\n",
    "    for cls in range(NUM_CLASSES):\n",
    "        key = (fold, cls)\n",
    "        if key not in data:\n",
    "            continue\n",
    "\n",
    "        df = pd.DataFrame(data[key])\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(df[\"epoch\"], df[\"f1\"], label=\"F1\")\n",
    "        plt.plot(df[\"epoch\"], df[\"precision\"], label=\"Precision\")\n",
    "        plt.plot(df[\"epoch\"], df[\"recall\"], label=\"Recall\")\n",
    "        plt.plot(df[\"epoch\"], df[\"accuracy\"], label=\"Accuracy\")\n",
    "\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.title(f\"Fold {fold} — Flood Level {cls}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e05c60-9def-4165-a56c-5b5a05cbf532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# FINAL NGBoost ↔ sklearn compatibility patch (FIT + PREDICT)\n",
    "# ==========================================================\n",
    "# Patches BOTH check_X_y and check_array inside NGBoost\n",
    "# to support sklearn versions without `ensure_all_finite`.\n",
    "\n",
    "import inspect\n",
    "import sklearn.utils.validation as _sk_val\n",
    "import ngboost.ngboost as _ngb_mod\n",
    "\n",
    "# ---------- check_X_y patch ----------\n",
    "if \"ensure_all_finite\" not in inspect.signature(_sk_val.check_X_y).parameters:\n",
    "\n",
    "    _orig_check_X_y = _sk_val.check_X_y\n",
    "\n",
    "    def _check_X_y_compat(\n",
    "        X, y,\n",
    "        accept_sparse=False,\n",
    "        accept_large_sparse=True,\n",
    "        dtype=\"numeric\",\n",
    "        order=None,\n",
    "        copy=False,\n",
    "        force_all_finite=True,\n",
    "        ensure_all_finite=None,\n",
    "        ensure_2d=True,\n",
    "        allow_nd=False,\n",
    "        multi_output=False,\n",
    "        ensure_min_samples=1,\n",
    "        ensure_min_features=1,\n",
    "        y_numeric=False,\n",
    "        estimator=None,\n",
    "    ):\n",
    "        if ensure_all_finite is not None:\n",
    "            force_all_finite = ensure_all_finite\n",
    "\n",
    "        return _orig_check_X_y(\n",
    "            X, y,\n",
    "            accept_sparse=accept_sparse,\n",
    "            accept_large_sparse=accept_large_sparse,\n",
    "            dtype=dtype,\n",
    "            order=order,\n",
    "            copy=copy,\n",
    "            force_all_finite=force_all_finite,\n",
    "            ensure_2d=ensure_2d,\n",
    "            allow_nd=allow_nd,\n",
    "            multi_output=multi_output,\n",
    "            ensure_min_samples=ensure_min_samples,\n",
    "            ensure_min_features=ensure_min_features,\n",
    "            y_numeric=y_numeric,\n",
    "            estimator=estimator,\n",
    "        )\n",
    "\n",
    "    _sk_val.check_X_y = _check_X_y_compat\n",
    "    _ngb_mod.check_X_y = _check_X_y_compat\n",
    "\n",
    "\n",
    "# ---------- check_array patch ----------\n",
    "if \"ensure_all_finite\" not in inspect.signature(_sk_val.check_array).parameters:\n",
    "\n",
    "    _orig_check_array = _sk_val.check_array\n",
    "\n",
    "    def _check_array_compat(\n",
    "        array,\n",
    "        accept_sparse=False,\n",
    "        accept_large_sparse=True,\n",
    "        dtype=\"numeric\",\n",
    "        order=None,\n",
    "        copy=False,\n",
    "        force_all_finite=True,\n",
    "        ensure_all_finite=None,\n",
    "        ensure_2d=True,\n",
    "        allow_nd=False,\n",
    "        ensure_min_samples=1,\n",
    "        ensure_min_features=1,\n",
    "        estimator=None,\n",
    "        input_name=\"\",\n",
    "    ):\n",
    "        if ensure_all_finite is not None:\n",
    "            force_all_finite = ensure_all_finite\n",
    "\n",
    "        return _orig_check_array(\n",
    "            array,\n",
    "            accept_sparse=accept_sparse,\n",
    "            accept_large_sparse=accept_large_sparse,\n",
    "            dtype=dtype,\n",
    "            order=order,\n",
    "            copy=copy,\n",
    "            force_all_finite=force_all_finite,\n",
    "            ensure_2d=ensure_2d,\n",
    "            allow_nd=allow_nd,\n",
    "            ensure_min_samples=ensure_min_samples,\n",
    "            ensure_min_features=ensure_min_features,\n",
    "            estimator=estimator,\n",
    "            input_name=input_name,\n",
    "        )\n",
    "\n",
    "    _sk_val.check_array = _check_array_compat\n",
    "    _ngb_mod.check_array = _check_array_compat\n",
    "\n",
    "\n",
    "print(\"[PATCH] NGBoost check_X_y + check_array compatibility applied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7eabbd-424b-49d5-9ea9-7ee68c6eba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpts_meta = train_kfold_hybrid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb94a6b-bc22-4a59-96da-f57f34a0baa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ensemble_eval_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68eba651-7e19-4807-9733-be6b47721ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 — FINAL UPDATED INFERENCE CELL (ConvNeXt + NGBoost-compatible)\n",
    "# Updated to use ConvNeXtBackbone (convnext_base) and NGBoost fusion checkpoints\n",
    "from ultralytics import YOLO as UltralyticsYOLO\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# NGBoost (for unpickling models)\n",
    "from ngboost import NGBClassifier\n",
    "\n",
    "# -----------------------\n",
    "# Safety / missing-global bootstrap\n",
    "# -----------------------\n",
    "# Ensure CLASS_HEIGHT_PRIORS_CM and DEFAULT_CLASS_HEIGHT are present (prevents NameError after kernel restart)\n",
    "try:\n",
    "    CLASS_HEIGHT_PRIORS_CM  # noqa: F821\n",
    "except NameError:\n",
    "    CLASS_HEIGHT_PRIORS_CM = {\n",
    "        \"person\":170, \"car\":150, \"bus\":300, \"truck\":300,\n",
    "        \"bicycle\":100, \"motorcycle\":110,\n",
    "    }\n",
    "try:\n",
    "    DEFAULT_CLASS_HEIGHT  # noqa: F821\n",
    "except NameError:\n",
    "    DEFAULT_CLASS_HEIGHT = 150\n",
    "\n",
    "# -----------------------\n",
    "# Helper: load YOLO models\n",
    "# -----------------------\n",
    "def load_yolo_models():\n",
    "    det = UltralyticsYOLO(YOLO_DET_PATH)\n",
    "    pose = UltralyticsYOLO(YOLO_POSE_PATH) if os.path.exists(YOLO_POSE_PATH) else None\n",
    "    wseg = UltralyticsYOLO(YOLO_WSEG_PATH) if os.path.exists(YOLO_WSEG_PATH) else None\n",
    "    return det, pose, wseg\n",
    "\n",
    "# ----------------------------------------\n",
    "# Water segmentation (defensive)\n",
    "# ----------------------------------------\n",
    "def run_water_seg(wseg_model, image, conf=0.30):\n",
    "    try:\n",
    "        if wseg_model is None:\n",
    "            return np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "        res = wseg_model.predict(image, imgsz=1024, conf=conf, verbose=False)[0]\n",
    "        if getattr(res, \"masks\", None) is None:\n",
    "            return np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "        mask = res.masks.data.cpu().numpy()\n",
    "        if mask.ndim == 3:\n",
    "            mask = (mask > 0.5).any(axis=0)\n",
    "        else:\n",
    "            mask = (mask > 0.5)\n",
    "        return mask.astype(np.uint8)\n",
    "    except Exception:\n",
    "        return np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "\n",
    "# ----------------------------------------\n",
    "# Pose extraction (defensive)\n",
    "# ----------------------------------------\n",
    "def extract_poses(pose_model, image, conf=0.25):\n",
    "    if pose_model is None:\n",
    "        return []\n",
    "    try:\n",
    "        res = pose_model.predict(image, imgsz=640, conf=conf, verbose=False)[0]\n",
    "        if getattr(res, \"keypoints\", None) is None:\n",
    "            return []\n",
    "        arr = res.keypoints.data.cpu().numpy()\n",
    "        return [arr[i] for i in range(arr.shape[0])]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# ----------------------------------------\n",
    "# Base physics features (kept as before)\n",
    "# ----------------------------------------\n",
    "def compute_base_features(box, mask, img_w, img_h):\n",
    "    x1,y1,x2,y2 = map(int,box)\n",
    "    bw = max(1, x2-x1); bh = max(1, y2-y1)\n",
    "    box_area = bw*bh\n",
    "\n",
    "    submask = mask[y1:y2, x1:x2] if (y2>y1 and x2>x1) else np.zeros((0,0), dtype=np.uint8)\n",
    "    water_pixels = int(submask.sum())\n",
    "    water_area_frac = water_pixels / (box_area + 1e-9)\n",
    "\n",
    "    ys = np.where(submask>0)[0]\n",
    "    if len(ys)>0:\n",
    "        top_local = ys.min() + y1\n",
    "        water_height_frac = (y2 - top_local) / (bh + 1e-9)\n",
    "    else:\n",
    "        top_local = y2\n",
    "        water_height_frac = 0.0\n",
    "\n",
    "    if submask.size>0:\n",
    "        sm = (submask*255).astype(np.uint8)\n",
    "        gx = cv2.Sobel(sm, cv2.CV_32F, 1,0, ksize=3)\n",
    "        gy = cv2.Sobel(sm, cv2.CV_32F, 0,1, ksize=3)\n",
    "        grad = np.sqrt(gx*gx + gy*gy)\n",
    "        sgf_mean = float(grad.mean())\n",
    "        sgf_max = float(grad.max())\n",
    "    else:\n",
    "        sgf_mean, sgf_max = 0.0, 0.0\n",
    "\n",
    "    water_top_y = int(top_local)\n",
    "\n",
    "    return {\n",
    "        \"box_x1\": x1, \"box_y1\": y1, \"box_x2\": x2, \"box_y2\": y2,\n",
    "        \"box_w\": bw, \"box_h\": bh, \"box_area\": box_area,\n",
    "        \"water_pixels\": water_pixels, \"water_area_frac\": water_area_frac,\n",
    "        \"water_top_y\": water_top_y, \"water_height_frac\": water_height_frac,\n",
    "        \"sgf_mean\": sgf_mean, \"sgf_max\": sgf_max, \"WOMI\": water_area_frac,\n",
    "        \"submergence_ratio\": water_height_frac,\n",
    "        \"bbox_ratio\": bh/(bw+1e-9),\n",
    "        \"water_frac_ratio\": water_area_frac,\n",
    "        \"waterline_norm\": water_top_y/(img_h+1e-9)\n",
    "    }\n",
    "\n",
    "# small safe nanmean used by pose helpers\n",
    "def nanmean_safe(values, fallback):\n",
    "    arr = []\n",
    "    for v in values:\n",
    "        if v is None:\n",
    "            arr.append(np.nan)\n",
    "        else:\n",
    "            try:\n",
    "                arr.append(float(v))\n",
    "            except:\n",
    "                arr.append(np.nan)\n",
    "    out = np.nanmean(arr)\n",
    "    if np.isnan(out):\n",
    "        return fallback\n",
    "    return out\n",
    "\n",
    "# ----------------------------------------\n",
    "# Person semantics (kept as before)\n",
    "# ----------------------------------------\n",
    "def person_semantics(pose, water_top_y, box):\n",
    "    x1,y1,x2,y2 = map(int, box)\n",
    "    bh = max(1, y2-y1)\n",
    "\n",
    "    ankle_b = y1 + 0.95*bh\n",
    "    knee_b  = y1 + 0.75*bh\n",
    "    hip_b   = y1 + 0.55*bh\n",
    "    chest_b = y1 + 0.35*bh\n",
    "    head_b  = y1 + 0.15*bh\n",
    "\n",
    "    mid_thigh_b    = y1 + 0.80*bh\n",
    "    upper_thigh_b  = y1 + 0.70*bh\n",
    "    lower_waist_b  = y1 + 0.60*bh\n",
    "\n",
    "    ankle, knee, hip, chest, head = ankle_b, knee_b, hip_b, chest_b, head_b\n",
    "    mid_thigh, upper_thigh, lower_waist = mid_thigh_b, upper_thigh_b, lower_waist_b\n",
    "\n",
    "    if pose is not None:\n",
    "        try:\n",
    "            conf = pose[:,2]\n",
    "        except Exception:\n",
    "            conf = np.zeros(pose.shape[0])\n",
    "\n",
    "        def gety(idx):\n",
    "            try:\n",
    "                if conf[idx] > 0.25:\n",
    "                    return float(pose[idx,1])\n",
    "            except Exception:\n",
    "                return None\n",
    "\n",
    "        ankle = nanmean_safe([gety(15), gety(16)], ankle_b)\n",
    "        knee  = nanmean_safe([gety(13), gety(14)], knee_b)\n",
    "        hip   = nanmean_safe([gety(11), gety(12)], hip_b)\n",
    "\n",
    "        chest = nanmean_safe([gety(5), gety(6)], chest_b)\n",
    "        head  = nanmean_safe([gety(0)], head_b)\n",
    "\n",
    "    flags = {\n",
    "        \"person_sub_ankle\": int(water_top_y > ankle),\n",
    "        \"person_sub_knee\": int(water_top_y > knee),\n",
    "        \"person_sub_hip\": int(water_top_y > hip),\n",
    "        \"person_sub_chest\": int(water_top_y > chest),\n",
    "        \"person_sub_head\": int(water_top_y > head),\n",
    "\n",
    "        \"person_sub_mid_thigh\": int(water_top_y > mid_thigh),\n",
    "        \"person_sub_upper_thigh\": int(water_top_y > upper_thigh),\n",
    "        \"person_sub_lower_waist\": int(water_top_y > lower_waist),\n",
    "    }\n",
    "\n",
    "    denom = head - ankle\n",
    "    if denom <= 0:\n",
    "        depth = 0.0\n",
    "        depth_fine = 0.0\n",
    "    else:\n",
    "        depth = (water_top_y - ankle) / denom\n",
    "        depth_fine = (water_top_y - lower_waist) / denom\n",
    "        depth = float(np.clip(depth, 0.0, 1.0))\n",
    "        depth_fine = float(np.clip(depth_fine, 0.0, 1.0))\n",
    "\n",
    "    flags[\"person_depth_norm\"] = depth\n",
    "    flags[\"person_depth_fine_ratio\"] = depth_fine\n",
    "    return flags\n",
    "\n",
    "# ----------------------------------------\n",
    "# Vehicle semantics (kept as before)\n",
    "# ----------------------------------------\n",
    "def vehicle_semantics(box, water_top_y, cname):\n",
    "    cname = cname.lower()\n",
    "    x1,y1,x2,y2 = map(int, box)\n",
    "    bh = max(1, y2-y1)\n",
    "\n",
    "    wheel = y1 + int(0.85*bh)\n",
    "    mid   = y1 + int(0.50*bh)\n",
    "    window= y1 + int(0.65*bh)\n",
    "    roof  = y1\n",
    "\n",
    "    flags = {}\n",
    "\n",
    "    if \"car\" in cname:\n",
    "        flags[\"car_sub_wheel\"] = int(water_top_y > wheel)\n",
    "        flags[\"car_sub_mid\"] = int(water_top_y > mid)\n",
    "        flags[\"car_sub_window\"] = int(water_top_y > window)\n",
    "        flags[\"car_sub_roof\"] = int(water_top_y > roof)\n",
    "        flags[\"car_sub_half_door\"] = int(water_top_y > (y1 + int(0.40*bh)))\n",
    "        flags[\"car_depth_norm\"] = float(np.clip((water_top_y - wheel)/max(1, roof-wheel), 0, 1))\n",
    "\n",
    "    if \"truck\" in cname or \"bus\" in cname:\n",
    "        flags[\"truck_sub_wheel\"] = int(water_top_y > wheel)\n",
    "        flags[\"truck_sub_mid\"] = int(water_top_y > mid)\n",
    "        flags[\"truck_sub_window\"] = int(water_top_y > window)\n",
    "        flags[\"truck_sub_roof\"] = int(water_top_y > roof)\n",
    "        flags[\"truck_sub_hubcap\"] = int(water_top_y > (y1 + int(0.92*bh)))\n",
    "        flags[\"truck_depth_norm\"] = float(np.clip((water_top_y - wheel)/max(1, roof-wheel), 0, 1))\n",
    "\n",
    "    if \"motorcycle\" in cname or \"motorbike\" in cname:\n",
    "        wheel2 = y1 + int(0.90*bh)\n",
    "        seat2  = y1 + int(0.60*bh)\n",
    "        handle = y1 + int(0.30*bh)\n",
    "        flags[\"motorcycle_sub_wheel\"] = int(water_top_y > wheel2)\n",
    "        flags[\"motorcycle_sub_seat\"] = int(water_top_y > seat2)\n",
    "        flags[\"motorcycle_sub_handle\"] = int(water_top_y > handle)\n",
    "        flags[\"motorcycle_sub_engine_level\"] = int(water_top_y > (y1 + int(0.75*bh)))\n",
    "        flags[\"motorcycle_depth_norm\"] = float(np.clip((water_top_y - wheel2)/max(1, handle-wheel2), 0, 1))\n",
    "\n",
    "    if \"bicycle\" in cname or \"cycle\" in cname:\n",
    "        wheel2 = y1 + int(0.90*bh)\n",
    "        seat2  = y1 + int(0.60*bh)\n",
    "        handle = y1 + int(0.30*bh)\n",
    "        flags[\"bicycle_sub_wheel\"] = int(water_top_y > wheel2)\n",
    "        flags[\"bicycle_sub_seat\"] = int(water_top_y > seat2)\n",
    "        flags[\"bicycle_sub_handle\"] = int(water_top_y > handle)\n",
    "        flags[\"bicycle_depth_norm\"] = float(np.clip((water_top_y - wheel2)/max(1, handle-wheel2), 0, 1))\n",
    "\n",
    "    return flags\n",
    "\n",
    "# ----------------------------------------\n",
    "# Generic semantics\n",
    "# ----------------------------------------\n",
    "def generic_semantics(subratio):\n",
    "    return {\n",
    "        \"obj_sub_20pct\": int(subratio > 0.20),\n",
    "        \"obj_sub_50pct\": int(subratio > 0.50),\n",
    "        \"obj_sub_80pct\": int(subratio > 0.80)\n",
    "    }\n",
    "\n",
    "# safe estimate depth using priors (uses bootstrap globals)\n",
    "def estimate_depth_cm(subratio, cname):\n",
    "    try:\n",
    "        sr = 0.0 if subratio is None else float(subratio)\n",
    "        if np.isnan(sr): sr = 0.0\n",
    "    except Exception:\n",
    "        sr = 0.0\n",
    "    cname_l = str(cname).strip().lower()\n",
    "    ref = CLASS_HEIGHT_PRIORS_CM.get(cname_l, DEFAULT_CLASS_HEIGHT)\n",
    "    return float(ref * max(0.0, min(1.0, sr))), float(ref)\n",
    "\n",
    "# nearest pose (copied defensive)\n",
    "def nearest_pose(box, poses):\n",
    "    if len(poses)==0:\n",
    "        return None\n",
    "    x1,y1,x2,y2 = box\n",
    "    cx=(x1+x2)/2; cy=(y1+y2)/2\n",
    "    best=None; best_d=1e12\n",
    "    for p in poses:\n",
    "        try:\n",
    "            xs=p[:,0]; ys=p[:,1]\n",
    "            xs=xs[~np.isnan(xs)]; ys=ys[~np.isnan(ys)]\n",
    "            if len(xs)==0:\n",
    "                px,py=cx,cy\n",
    "            else:\n",
    "                px,py=xs.mean(), ys.mean()\n",
    "        except Exception:\n",
    "            px,py=cx,cy\n",
    "        d=(px-cx)**2+(py-cy)**2\n",
    "        if d<best_d:\n",
    "            best_d=d; best=p\n",
    "    return best\n",
    "\n",
    "# =====================================================\n",
    "# Build feature vector matching ALL_FEATURES\n",
    "# =====================================================\n",
    "def build_44_features(full_rgb, box, cname, wseg_model, pose_model):\n",
    "    h,w = full_rgb.shape[:2]\n",
    "\n",
    "    mask = run_water_seg(wseg_model, full_rgb)\n",
    "    base = compute_base_features(box, mask, w, h)\n",
    "\n",
    "    poses = extract_poses(pose_model, full_rgb)\n",
    "    pose_for_obj = nearest_pose(box, poses) if len(poses)>0 else None\n",
    "\n",
    "    sem = {}\n",
    "\n",
    "    if any(t in cname for t in [\"person\",\"human\",\"man\",\"woman\",\"child\"]):\n",
    "        sem.update(person_semantics(pose_for_obj, base[\"water_top_y\"], box))\n",
    "\n",
    "    if any(t in cname for t in [\"car\",\"bus\",\"truck\",\"motorbike\",\"motorcycle\",\"bicycle\",\"cycle\"]):\n",
    "        sem.update(vehicle_semantics(box, base[\"water_top_y\"], cname))\n",
    "\n",
    "    sem.update(generic_semantics(base[\"submergence_ratio\"]))\n",
    "\n",
    "    est_cm, ref_cm = estimate_depth_cm(base[\"submergence_ratio\"], cname)\n",
    "\n",
    "    row = {}\n",
    "\n",
    "    # base features\n",
    "    for k,v in base.items(): row[k] = float(v)\n",
    "    row[\"estimated_depth_cm\"] = float(est_cm)\n",
    "    row[\"ref_height_cm\"] = float(ref_cm)\n",
    "    row[\"physics_residual\"] = 0.0\n",
    "\n",
    "    # ensure all exist (ALL_FEATURES expected to be defined in earlier cells)\n",
    "    for f in ALL_FEATURES:\n",
    "        if f not in row:\n",
    "            row[f] = 0.0\n",
    "\n",
    "    # apply semantics\n",
    "    for k,v in sem.items():\n",
    "        row[k] = float(v)\n",
    "\n",
    "    # build vector\n",
    "    return np.array([row[k] for k in ALL_FEATURES], dtype=np.float32)\n",
    "\n",
    "# =====================================================\n",
    "# INFERENCE FUNCTION (ConvNeXt + NGBoost-aware)\n",
    "# =====================================================\n",
    "def infer_image(image_path, out_csv):\n",
    "    meta_path = Path(OUT_DIR)/\"folds_meta.pkl\"\n",
    "    if not meta_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing fold metadata at {meta_path} — run training first.\")\n",
    "    meta = joblib.load(meta_path)\n",
    "    if len(meta)==0:\n",
    "        raise RuntimeError(\"No fold metadata found. Train first.\")\n",
    "\n",
    "    # instantiate backbone as ConvNeXt (ensure ConvNeXtBackbone is defined in Cell 4)\n",
    "    try:\n",
    "        backbone = ConvNeXtBackbone(variant=\"convnext_base\", pretrained=True, img_size=IMAGE_SIZE).to(DEVICE)\n",
    "    except NameError:\n",
    "        raise RuntimeError(\"ConvNeXtBackbone is not defined. Please ensure you replaced Cell 4 with the ConvNeXtBackbone implementation.\")\n",
    "\n",
    "    backbone.eval()\n",
    "\n",
    "    det_model, pose_model, wseg_model = load_yolo_models()\n",
    "\n",
    "    img_bgr = cv2.imread(image_path)\n",
    "    if img_bgr is None:\n",
    "        raise FileNotFoundError(image_path)\n",
    "    img_rgb = img_bgr[:,:,::-1]\n",
    "\n",
    "    det_res = det_model.predict(img_rgb, imgsz=1024, conf=0.25)[0]\n",
    "    names = det_model.model.names\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # Load fold models (defensive). We create per-fold fusion objects consistent with saved ckpt.\n",
    "    fold_models=[]\n",
    "    for fo in meta:\n",
    "        # load state dict first to infer emb_dim expected by fusion ckpt\n",
    "        state = torch.load(fo[\"fusion_ckpt\"], map_location=DEVICE)\n",
    "        # try to infer in_features of first linear in mlp (our FusionMLP uses mlp[0].weight)\n",
    "        inferred_emb_dim = None\n",
    "        try:\n",
    "            w0 = None\n",
    "            # common key names: 'mlp.0.weight' or similar - try heuristics\n",
    "            if \"mlp.0.weight\" in state:\n",
    "                w0 = state[\"mlp.0.weight\"]\n",
    "            else:\n",
    "                for k in state.keys():\n",
    "                    if k.endswith(\".weight\") and k.startswith(\"mlp.\"):\n",
    "                        w0 = state[k]; break\n",
    "            if w0 is not None:\n",
    "                in_features = w0.shape[1]  # equals (hidden, emb_dim + NUM_CLASSES)\n",
    "                inferred_emb_dim = int(in_features - NUM_CLASSES)\n",
    "                if inferred_emb_dim <= 0:\n",
    "                    inferred_emb_dim = None\n",
    "        except Exception:\n",
    "            inferred_emb_dim = None\n",
    "\n",
    "        # if we couldn't infer, fallback to backbone.out_dim\n",
    "        desired_emb_dim = inferred_emb_dim if inferred_emb_dim is not None else getattr(backbone, \"out_dim\", None)\n",
    "\n",
    "        # create fusion with correct emb dim\n",
    "        fusion = FusionMLP(emb_dim=desired_emb_dim, xgb_prob_dim=NUM_CLASSES).to(DEVICE)\n",
    "        try:\n",
    "            fusion.load_state_dict(state)\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                fusion.load_state_dict(state, strict=False)\n",
    "            except Exception:\n",
    "                raise RuntimeError(f\"Failed to load fusion checkpoint {fo['fusion_ckpt']} into FusionMLP. Error: {e}\")\n",
    "\n",
    "        fusion.eval()\n",
    "\n",
    "        # load NGBoost model and scaler\n",
    "        ngb_model = joblib.load(fo[\"ngb_model\"])\n",
    "        scaler = joblib.load(fo[\"scaler_path\"])\n",
    "\n",
    "        # if backbone embedding dim differs from fusion expected emb dim, create a linear adapter\n",
    "        adapter = None\n",
    "        bb_out_dim = getattr(backbone, \"out_dim\", None)\n",
    "        if bb_out_dim is not None and desired_emb_dim is not None and bb_out_dim != desired_emb_dim:\n",
    "            adapter = torch.nn.Linear(bb_out_dim, desired_emb_dim).to(DEVICE)\n",
    "            adapter.eval()\n",
    "            print(f\"[infer_image] created adapter: project {bb_out_dim} -> {desired_emb_dim} for fold {fo.get('fold', '??')}\")\n",
    "\n",
    "        fold_models.append({\"fusion\":fusion, \"ngb\":ngb_model, \"scaler\":scaler, \"adapter\":adapter})\n",
    "\n",
    "    # Iterate boxes\n",
    "    for i in range(len(det_res.boxes)):\n",
    "        cls_id = int(det_res.boxes.cls[i].item())\n",
    "        cname = names[cls_id].lower()\n",
    "\n",
    "        if cname not in [\"person\",\"car\",\"motorcycle\",\"truck\",\"bus\",\"bicycle\"]:\n",
    "            continue\n",
    "\n",
    "        x1,y1,x2,y2 = det_res.boxes.xyxy[i].cpu().numpy().astype(int)\n",
    "        x1 = max(0, x1); y1 = max(0, y1)\n",
    "        x2 = max(0, x2); y2 = max(0, y2)\n",
    "        box=[x1,y1,x2,y2]\n",
    "\n",
    "        crop = img_rgb[y1:y2, x1:x2]\n",
    "        if crop.size==0:\n",
    "            img_t = val_transform(np.zeros((IMAGE_SIZE,IMAGE_SIZE,3),dtype=np.uint8)).unsqueeze(0).to(DEVICE)\n",
    "        else:\n",
    "            img_t = val_transform(crop).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            emb = backbone(img_t)  # (1, bb_out_dim) or (1, D)\n",
    "            if emb.dim() == 4:\n",
    "                emb = torch.nn.functional.adaptive_avg_pool2d(emb, 1).reshape(emb.shape[0], -1)\n",
    "            emb_np = emb.cpu().numpy()\n",
    "\n",
    "        # Build full feature vector\n",
    "        feat_vec = build_44_features(img_rgb, box, cname, wseg_model, pose_model)\n",
    "\n",
    "        # ensemble prediction\n",
    "        probs = np.zeros((1,NUM_CLASSES), dtype=np.float32)\n",
    "        for fm in fold_models:\n",
    "            Xs = fm[\"scaler\"].transform(feat_vec.reshape(1,-1))\n",
    "            xp = fm[\"ngb\"].predict_proba(Xs)  # (1, NUM_CLASSES)\n",
    "\n",
    "            emb_t = torch.from_numpy(emb_np).float().to(DEVICE)  # (1, bb_out_dim)\n",
    "            if fm.get(\"adapter\", None) is not None:\n",
    "                with torch.no_grad():\n",
    "                    emb_t = fm[\"adapter\"](emb_t)\n",
    "\n",
    "            xp_t = torch.from_numpy(xp).float().to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = fm[\"fusion\"](emb_t, xp_t)\n",
    "                p = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "            probs += p\n",
    "\n",
    "        probs /= max(1, len(fold_models))\n",
    "\n",
    "        # --- apply meta-classifier filter for class-4 (if available) ---\n",
    "        try:\n",
    "            meta_clf_path = Path(OUT_DIR) / \"meta_clf_class4.pkl\"\n",
    "            chosen_thr = 0.75  # keep in sync with your validation choice\n",
    "\n",
    "            if meta_clf_path.exists():\n",
    "                meta_clf = joblib.load(meta_clf_path)\n",
    "\n",
    "                # fusion features\n",
    "                fusion_prob4 = float(probs[0, 4])\n",
    "                fusion_conf_val = float(probs.max())\n",
    "                ngb_entropy_val = float(-np.sum(np.clip(probs, 1e-12, 1.0) * np.log(np.clip(probs, 1e-12, 1.0))))\n",
    "\n",
    "                # proto_sim4: average similarity to prototypes (use first available prototypes file)\n",
    "                proto_sim4_val = 0.0\n",
    "                proto_files = []\n",
    "                for fo in meta:\n",
    "                    cand = Path(fo[\"fusion_ckpt\"]).parent / f\"prototypes_fold{fo['fold']}.npz\"\n",
    "                    if cand.exists():\n",
    "                        proto_files.append(cand)\n",
    "                if len(proto_files) > 0:\n",
    "                    P = np.load(proto_files[0], allow_pickle=True)\n",
    "                    prototypes = P[\"prototypes\"].item()\n",
    "                    if 4 in prototypes:\n",
    "                        prot4 = np.array(prototypes[4], dtype=np.float32)\n",
    "                        prot4 = prot4 / (np.linalg.norm(prot4) + 1e-12)\n",
    "                        emb_vec = emb_np.reshape(-1)  # (D,)\n",
    "                        emb_vec = emb_vec / (np.linalg.norm(emb_vec) + 1e-12)\n",
    "                        proto_sim4_val = float(np.dot(emb_vec, prot4))\n",
    "\n",
    "                # assemble meta feature vector (shape (1,4))\n",
    "                X_meta_box = np.array([[fusion_prob4, fusion_conf_val, proto_sim4_val, ngb_entropy_val]], dtype=np.float32)\n",
    "                meta_conf = meta_clf.predict_proba(X_meta_box)[:, 1][0]\n",
    "\n",
    "                # if meta says this predicted-4 should be demoted, zero it out and renormalize\n",
    "                if int(probs.argmax(axis=1)[0]) == 4 and meta_conf < chosen_thr:\n",
    "                    probs[0, 4] = 0.0\n",
    "                    s = probs.sum()\n",
    "                    if s <= 0:\n",
    "                        probs[0] = 1.0 / probs.shape[1]\n",
    "                    else:\n",
    "                        probs = probs / s\n",
    "        except Exception:\n",
    "            # defensive: if anything goes wrong, skip meta-filter and continue\n",
    "            pass\n",
    "\n",
    "        pred = int(probs.argmax(axis=1)[0]) if probs.size>0 else 0\n",
    "\n",
    "        row = {\n",
    "            \"image_path\": image_path,\n",
    "            \"class_name\": cname,\n",
    "            \"box_x1\": x1, \"box_y1\": y1,\n",
    "            \"box_x2\": x2, \"box_y2\": y2,\n",
    "            \"predicted_level\": pred\n",
    "        }\n",
    "\n",
    "        # Store full feature vector for debugging/analysis\n",
    "        for j,f in enumerate(ALL_FEATURES):\n",
    "            row[f] = float(feat_vec[j])\n",
    "            # --- Add per-class probabilities and predicted-prob column (NO other changes) ---\n",
    "            # (uses the `probs` and `pred` variables already computed above)\n",
    "            for k in range(NUM_CLASSES):\n",
    "                row[f\"prob_L{k}\"] = float(probs[0, k])\n",
    "            row[\"pred_prob\"] = float(probs[0, pred]) if 0 <= pred < NUM_CLASSES else 0.0\n",
    "            # --- end snippet ---\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    df=pd.DataFrame(rows)\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(\"Saved inference CSV:\", out_csv)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a0f643-4fd5-4af4-a395-f7b9f458c5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inf = infer_image(\n",
    "    \"/home/arnab/Desktop/yolo/data/Flood_model/test/1318_jpg.rf.8da76f24cfdcd4d19ccc4fe2f2147ddd.jpg\",\n",
    "    out_csv=f\"{OUT_DIR}/inference_1318.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ffa593-b733-4d00-9708-2dcedaa4d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 — Visualize flood-level predictions with bounding boxes (clean version: no LLM, no final-line overlay)\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def visualize_inference(image_path, df_inf, save_path=None, show_confidence=False, max_display_size=1200):\n",
    "    \"\"\"\n",
    "    Draw boxes from df_inf.\n",
    "    df_inf must contain: box_x1, box_y1, box_x2, box_y2, class_name, predicted_level,\n",
    "    and optionally pred_prob or prob_L* columns.\n",
    "    \"\"\"\n",
    "\n",
    "    if df_inf is None or len(df_inf) == 0:\n",
    "        raise ValueError(\"df_inf is empty or None — run infer_image() first.\")\n",
    "\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(image_path)\n",
    "\n",
    "    h0, w0 = img.shape[:2]\n",
    "\n",
    "    # simple color map for flood levels\n",
    "    cmap = {}\n",
    "    for lvl in range(0, 11):\n",
    "        t = lvl / 10.0\n",
    "        b = int((1.0 - t) * 0 + t * 0)\n",
    "        g = int((1.0 - t) * 200 + t * 50)\n",
    "        r = int((1.0 - t) * 0 + t * 200)\n",
    "        cmap[lvl] = (b, g, r)\n",
    "\n",
    "    img_out = img.copy()\n",
    "\n",
    "    for idx, row in df_inf.iterrows():\n",
    "        try:\n",
    "            x1 = int(row[\"box_x1\"])\n",
    "            y1 = int(row[\"box_y1\"])\n",
    "            x2 = int(row[\"box_x2\"])\n",
    "            y2 = int(row[\"box_y2\"])\n",
    "            cls_name = str(row.get(\"class_name\", \"obj\"))\n",
    "            level = int(row.get(\"predicted_level\", 0))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        x1 = max(0, min(x1, w0-1))\n",
    "        y1 = max(0, min(y1, h0-1))\n",
    "        x2 = max(0, min(x2, w0-1))\n",
    "        y2 = max(0, min(y2, h0-1))\n",
    "\n",
    "        color = cmap.get(level, (0, 255, 0))\n",
    "        cv2.rectangle(img_out, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "        label = f\"{cls_name} | L{level}\"\n",
    "        if show_confidence and \"pred_prob\" in df_inf.columns:\n",
    "            try:\n",
    "                prob = float(row.get(\"pred_prob\", 0.0))\n",
    "                label += f\" {prob:.2f}\"\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        text_y = max(12, y1 - 8)\n",
    "        cv2.putText(\n",
    "            img_out,\n",
    "            label,\n",
    "            (x1, text_y),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.6,\n",
    "            color,\n",
    "            2,\n",
    "            cv2.LINE_AA\n",
    "        )\n",
    "\n",
    "    # Resize for viewing\n",
    "    H, W = img_out.shape[:2]\n",
    "    scale = 1.0\n",
    "    if max(H, W) > max_display_size:\n",
    "        scale = max_display_size / max(H, W)\n",
    "        img_disp = cv2.resize(img_out, (int(W*scale), int(H*scale)))\n",
    "    else:\n",
    "        img_disp = img_out\n",
    "\n",
    "    # Convert BGR to RGB for display\n",
    "    img_rgb = img_disp[:, :, ::-1]\n",
    "\n",
    "    plt.figure(figsize=(12, 12 * (img_rgb.shape[0] / img_rgb.shape[1])))\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    if save_path:\n",
    "        cv2.imwrite(save_path, img_out)\n",
    "        print(\"Saved annotated image:\", save_path)\n",
    "\n",
    "    return img_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e005f8e-0eac-4b7f-9d99-564f83804491",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_inference(\n",
    "    \"/home/arnab/Desktop/yolo/data/Flood_model/test/1318_jpg.rf.8da76f24cfdcd4d19ccc4fe2f2147ddd.jpg\",\n",
    "    df_inf,\n",
    "    save_path=f\"{OUT_DIR}/1318_annotated.jpg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc9d9b3-a30e-4251-8f43-60c5a133a9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n",
    "\n",
    "# default: Load the model on the available device(s)\n",
    "model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen3-VL-4B-Instruct\", dtype=\"auto\", device_map=\"auto\")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-4B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16311043-f463-44d1-9952-40c8144500e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "image_path = \"/home/arnab/Desktop/yolo/data/Flood_model/yolov11_refined_balanced/object_label_csv2ConvNeXT_NGBoost_FShL+llm/hybrid_resnet50_xgb_fusion_v1/1318_annotated.jpg\"\n",
    "prompt = \"Give me the final single L value in one sentence, from this image's bounding boxes (general bounding box value), by seeing the person, car, motorcycle, bicycle, truck, bus in the water.\"\n",
    "image = Image.open(image_path)\n",
    "display(image)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": image_path,\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Move inputs to the same device as the model\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=1000)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs[\"input_ids\"], generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40211bf-761d-4ee0-a7ea-b4db557291ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
